{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "few-shot-graph-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Few-Shot Graph Classification\n",
        "\n",
        "Most of the graph classification task overlook the scarcity of labeled graph in many situations. To overcome this problem, *Few-Shot Learning* is started being used. It is a type of Machine Learning method where the training dataset contains limited information. The general practice is to feed the machine learning model with as much data as possible, since this leads to better predictions. However, few-shot learning aims to build accurate machine learning models with less training data. Few-Shot Learning, and in particular in this case Few-shot classification, aims to reduce the cost of gain and label a huge amount of data.\n",
        "\n",
        "*Which is the idea behind Few-Shot Learning*? (on graphs) Given graph data $\\mathcal{G} = \\{(G_1, \\mathbf{y}_1), ..., (G_n, \\mathbf{y}_n)\\}$, we split it into train, $\\{(G^{train}, \\mathbf{y}^{train})\\}$, and test dataset, $\\{(G^{test}, \\mathbf{y}^{test})\\}$. Notice that $\\mathbf{y}^{train}$ and $\\mathbf{y}^{test}$ must have no common classes. For training we use episodic training method, this means that at training stage the algorithm sample a so-called *Task*, i.e., a pair (*support* set, *query* set) where the support set is $D_{sup}^{train} = \\{(G_i^{train}, \\mathbf{y}_{i}^{train})\\}_{i=1}^s$, where $s = N \\times K$, while the query set is $D_{que}^{train} = \\{(G_i^{train}, \\mathbf{y}_{i}^{train})\\}_{i=1}^q$, where $q$ is the number of query data. Given labeled support data, the goal is to predict the labels of query data. Note that in a single task, support data and query data share the same class space. This is also called **N-way-K-shot** learning, where **N** is the number of sampled classes and **K** is the number of samples for each of the N classes. At test stage when performing classification tasks on unseen classes, we firstly fine tune the meta-learner on the support data of test classes, then report classification performance on the test query set.\n",
        "\n",
        "In the following, I'm going to present some approaches in few-shot Learning. First, a *Meta-Learning Framework* based on Fast Weight Adaptation, taken from the paper [Adaptive-Step Graph Meta-Learner for Few-Shot Graph Classification](https://arxiv.org/pdf/2003.08246.pdf) (Ning Ma et al.). Second, I'm going to compare it with different GDA (graph data augmentation) techniques used to enrich the dataset for the novel classes (i.e., those with the less amount of data) taken from a second paper named [Graph Data Augmentation for Graph Machine Learning: A Survey](https://arxiv.org/pdf/2202.08871.pdf) (Tong Zhao et al.)."
      ],
      "metadata": {
        "id": "6YcW8xJl1woa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modules and Constants"
      ],
      "metadata": {
        "id": "ezvmZRr_RhpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install pyyaml==5.4.1\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "WGkCr5T9HyrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import (\n",
        "    Any, Dict, List, Tuple, \n",
        "    Union, Generic, Optional,\n",
        "    TypeVar\n",
        ")\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from functools import wraps\n",
        "import plotly.graph_objects as go\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "\n",
        "import torch_geometric.data as gdata\n",
        "import torch_geometric.loader as gloader\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
        "from torch_geometric.nn.inits import uniform\n",
        "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
        "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
        "from torch_goemetric.utils import (\n",
        "    add_remaining_self_loops, \n",
        "    add_self_loops, \n",
        "    remove_self_loops,\n",
        "    softmax\n",
        ")\n",
        "\n",
        "from torch_scatter import scatter_add\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "WdiE2uITSaS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRIANGLES_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/TRIANGLES.zip\"\n",
        "COIL_DEL_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/COIL-DEL.zip\"\n",
        "R52_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/R52.zip\"\n",
        "LETTER_HIGH_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/Letter-High.zip\"\n",
        "\n",
        "DATASETS = {\n",
        "    \"TRIANGLES\"   : TRIANGLES_ZIP_URL, \n",
        "    \"COIL-DEL\"    : COIL_DEL_ZIP_URL, \n",
        "    \"R52\"         : R52_ZIP_URL, \n",
        "    \"Letter-High\" : LETTER_HIGH_ZIP_URL\n",
        "}\n",
        "\n",
        "T = TypeVar('T')\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DOWNLOAD_DATASET = True\n",
        "SAVE_PICLKE  = True\n",
        "EDGELIMIT_PRINT = 2000\n",
        "\n",
        "NUM_FEATURES = {\"TRIANGLES\": 1, \"R52\": 1, \"Letter-High\": 2, \"COIL-DEL\": 2}\n",
        "\n",
        "\n",
        "class ASMAMLConfig:\n",
        "    NHID = 128\n",
        "    POOLING_RATIO = 0.5\n",
        "    DROPOUT_RATIO = 0.3\n",
        "\n",
        "    OUTER_LR     = 0.001\n",
        "    INNER_LR     = 0.01\n",
        "    STOP_LR      = 0.0001\n",
        "    WEIGHT_DECAY = 1E-05\n",
        "\n",
        "    MAX_STEP      = 15\n",
        "    MIN_STEP      = 5\n",
        "    STEP_TEST     = 15\n",
        "    FLEXIBLE_STEP = True\n",
        "    STEP_PENALITY = 0.001\n",
        "    USE_SCORE     = True\n",
        "    USE_GRAD      = False\n",
        "    USE_LOSS      = True\n",
        "\n",
        "    TRAIN_SHOT         = 10   # K-shot for training set\n",
        "    VAL_SHOT           = 10   # K-shot for validation (or test) set\n",
        "    TRAIN_QUERY        = 15   # Number of query for the training set\n",
        "    VAL_QUERY          = 15   # Number of query for the validation (or test) set\n",
        "    TRAIN_WAY          = 3    # N-way for training set\n",
        "    TEST_WAY           = 3    # N-way for test set\n",
        "    VAL_EPISODE        = 200  # Number of episodes for validation\n",
        "    TRAIN_EPISODE      = 200  # Number of episodes for training\n",
        "    BATCH_PER_EPISODES = 5    # How many batch per episode\n",
        "    EPOCHS             = 500  # How many epochs\n",
        "    PATIENCE           = 35\n",
        "    GRAD_CLIP          = 5\n",
        "\n",
        "    # Stop Control configurations\n",
        "    STOP_CONTROL_INPUT_SIZE = 2\n",
        "    STOP_CONTROL_HIDDEN_SIZE = 20"
      ],
      "metadata": {
        "id": "7vboF7-3VcvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "qs070YovWwAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scandir(root_path: str) -> List[str]:\n",
        "    \"\"\"Recursively scan a directory looking for files\"\"\"\n",
        "    root_path = os.path.abspath(root_path)\n",
        "    content = []\n",
        "    for file in os.listdir(root_path):\n",
        "        new_path = os.path.join(root_path, file)\n",
        "        if os.path.isfile(new_path):\n",
        "            content.append(new_path)\n",
        "            continue\n",
        "        \n",
        "        content += scandir(new_path)\n",
        "    \n",
        "    return content\n",
        "\n",
        "\n",
        "def download_zipped_data(url: str, path2extract: str, dataset_name: str) -> List[str]:\n",
        "    \"\"\"Download and extract a ZIP file from URL. Return the content filename\"\"\"\n",
        "    logging.debug(f\"--- Downloading from {url} ---\")\n",
        "    response = requests.get(url)\n",
        "\n",
        "    abs_path2extract = os.path.abspath(path2extract)\n",
        "    zip_path = os.path.join(abs_path2extract, f\"{dataset_name}.zip\")\n",
        "    with open(zip_path, mode=\"wb\") as iofile:\n",
        "        iofile.write(response.content)\n",
        "\n",
        "    # Extract the file\n",
        "    logging.debug(\"--- Extracting files from the archive ---\")\n",
        "    with zipfile.ZipFile(zip_path, mode=\"r\") as zip_ref:\n",
        "        zip_ref.extractall(abs_path2extract)\n",
        "\n",
        "    logging.debug(f\"--- Removing {zip_path} ---\")\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    return scandir(os.path.join(path2extract, dataset_name))\n",
        "\n",
        "\n",
        "def delete_data_folder(path2delete: str) -> None:\n",
        "    \"\"\"Delete the folder containing data\"\"\"\n",
        "    logging.debug(\"--- Removing Content Data ---\")\n",
        "    shutil.rmtree(path2delete)\n",
        "    logging.debug(\"--- Removed Finished Succesfully ---\")"
      ],
      "metadata": {
        "id": "hiPgKuQqXDNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def elapsed_time(func):\n",
        "    \"\"\"Just a simple wrapper for counting elapsed time from start to end\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        logging.debug(\"Elapsed Time: {:.6f}\".format(end - start))\n",
        "    \n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "7DFVuisvXFHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_with_pickle(path2save: str, content: Any) -> None:\n",
        "    \"\"\"Save content inside a .pickle file denoted by path2save\"\"\"\n",
        "    path2save = path2save + \".pickle\" if \".pickle\" not in path2save else path2save\n",
        "    with open(path2save, mode=\"wb\") as iostream:\n",
        "        pickle.dump(content, iostream)\n",
        "\n",
        "\n",
        "def load_with_pickle(path2load: str) -> Any:\n",
        "    \"\"\"Load a content from a .pickle file\"\"\"\n",
        "    with open(path2load, mode=\"rb\") as iostream:\n",
        "        return pickle.load(iostream)"
      ],
      "metadata": {
        "id": "fN6UJb7jXJ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "JmagKA3bXL1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(G : Union[nx.Graph, nx.DiGraph], name: str) -> None:\n",
        "    \"\"\"\n",
        "    Plot a graph\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    graph : Union[nx.Graph, nx.DiGraph]\n",
        "        Just a nx.Graph object\n",
        "    name  : str\n",
        "        The name of the graph\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Getting the 3D Spring layout\n",
        "    layout = nx.spring_layout(G, dim=3, seed=18)\n",
        "    \n",
        "    # Getting nodes coordinate\n",
        "    x_nodes = [layout[i][0] for i in layout]  # x-coordinates of nodes\n",
        "    y_nodes = [layout[i][1] for i in layout]  # y-coordinates of nodes\n",
        "    z_nodes = [layout[i][2] for i in layout]  # z-coordinates of nodes\n",
        "    \n",
        "    # Getting a list of edges and create a list with coordinates\n",
        "    elist = G.edges()\n",
        "    x_edges, y_edges, z_edges = [], [], []\n",
        "    for edge in elist:\n",
        "        x_edges += [layout[edge[0]][0], layout[edge[1]][0], None]\n",
        "        y_edges += [layout[edge[0]][1], layout[edge[1]][1], None]\n",
        "        z_edges += [layout[edge[0]][2], layout[edge[1]][2], None]\n",
        "\n",
        "    colors = np.linspace(0, len(x_nodes))\n",
        "        \n",
        "    # Create a trace for the edges\n",
        "    etrace = go.Scatter3d(x=x_edges,\n",
        "                          y=y_edges,\n",
        "                          z=z_edges,\n",
        "                          mode='lines',\n",
        "                          line=dict(color='rgb(125,125,125)', width=1),\n",
        "                          hoverinfo='none'\n",
        "                         )\n",
        "    \n",
        "    # Create a trace for the nodes\n",
        "    ntrace = go.Scatter3d(x=x_nodes,\n",
        "                          y=y_nodes,\n",
        "                          z=z_nodes,\n",
        "                          mode='markers',\n",
        "                          marker=dict(\n",
        "                              symbol='circle',\n",
        "                              size=6,\n",
        "                              color=colors,\n",
        "                              colorscale='Viridis',\n",
        "                              line=dict(color='rgb(50,50,50)', width=.5)),\n",
        "                          text=list(layout.keys()),\n",
        "                          hoverinfo='text'\n",
        "                         )\n",
        "    \n",
        "    # Set the axis\n",
        "    axis = dict(showbackground=False,\n",
        "                showline=False,\n",
        "                zeroline=False,\n",
        "                showgrid=False,\n",
        "                showticklabels=False,\n",
        "                title='')\n",
        "    \n",
        "    # Create a layout for the plot\n",
        "    go_layout = go.Layout(title=f\"{name} Network Graph\",\n",
        "                          width=600,\n",
        "                          height=600,\n",
        "                          showlegend=False,\n",
        "                          scene=dict(xaxis=dict(axis),\n",
        "                                     yaxis=dict(axis),\n",
        "                                     zaxis=dict(axis)),\n",
        "                          margin=dict(t=100),\n",
        "                          hovermode='closest'\n",
        "                         )\n",
        "    \n",
        "    # Plot\n",
        "    data = [etrace, ntrace]\n",
        "    fig = go.Figure(data=data, layout=go_layout)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "hA0a8aVGX6kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_edge_indexes(data_list: List[gdata.Data]) -> List[gdata.Data]:\n",
        "    \"\"\"\n",
        "    Takes as input a bunch of :obj:`torch_geometric.data.Data` and renames\n",
        "    each edge node (x, y) from 1 to total number of nodes. For instance, if we have\n",
        "    this edge_index = [[1234, 1235, 1236, 1237], [1238, 1239, 1230,1241]] this became\n",
        "    egde_index = [[0, 1, 2, 3],[4, 5, 6, 7]] and so on. \n",
        "\n",
        "    :param data_list: the list of :obj:`torch_geometric.data.Data`\n",
        "    :return: a new list of data\n",
        "    \"\"\"\n",
        "    # First of all let's compute the total number of nodes overall\n",
        "    total_number_nodes = 0\n",
        "    for data in data_list:\n",
        "        total_number_nodes += data.x.shape[0]\n",
        "    \n",
        "    # Generate the new nodes\n",
        "    nodes = torch.arange(0, total_number_nodes)\n",
        "    \n",
        "    # Takes the old nodes from the edge_index attribute\n",
        "    old_nodes = None\n",
        "    for data in data_list:\n",
        "        x, y = data.edge_index\n",
        "        x = torch.hstack((x, y)).unique(sorted=False)\n",
        "        \n",
        "        if old_nodes is None:\n",
        "            old_nodes = x\n",
        "            continue\n",
        "    \n",
        "        old_nodes = torch.hstack((old_nodes, x))\n",
        "    \n",
        "    # Create mapping from old to new nodes\n",
        "    mapping = dict(zip(old_nodes.tolist(), nodes.tolist()[:old_nodes.shape[0]]))\n",
        "    \n",
        "    # Finally, map the new nodes\n",
        "    for data in data_list:\n",
        "        x, y = data.edge_index\n",
        "        new_x = torch.tensor(list(map(lambda x: mapping[x], x.tolist())), dtype=x.dtype, device=x.device)\n",
        "        new_y = torch.tensor(list(map(lambda y: mapping[y], y.tolist())), dtype=y.dtype, device=y.device)\n",
        "        new_edge_index = torch.vstack((new_x, new_y))\n",
        "        data.edge_index = new_edge_index\n",
        "    \n",
        "    return data_list\n",
        "\n",
        "\n",
        "def data_batch_collate(data_list: List[gdata.Data]) -> gdata.Data:\n",
        "    \"\"\"\n",
        "    Takes as input a list of data and create a new :obj:`torch_geometric.data.Data`\n",
        "    collating all together. This is a replacement for torch_geometric.data.Batch.from_data_list\n",
        "\n",
        "    :param data_list: a list of torch_geometric.data.Data objects\n",
        "    :return: a new torch_geometric.data.Data object\n",
        "    \"\"\"\n",
        "    x = None\n",
        "    edge_index = None\n",
        "    batch = []\n",
        "    num_graphs = 0\n",
        "    y = None\n",
        "\n",
        "    # Do a shuffle of the data\n",
        "    random.shuffle(data_list)\n",
        "    \n",
        "    for i_data, data in enumerate(data_list):\n",
        "        x = data.x if x is None else torch.vstack((x, data.x))\n",
        "        edge_index = data.edge_index if edge_index is None else torch.hstack((edge_index, data.edge_index))\n",
        "        batch += [i_data] * data.x.shape[0]\n",
        "        num_graphs += 1\n",
        "        y = data.y if y is None else torch.hstack((y, data.y))\n",
        "\n",
        "    # Create a mapping between y and a range(0, num_classes_of_y)\n",
        "    # First we need to compute how many classes do we have\n",
        "    num_classes = y.unique().shape[0]\n",
        "    classes = list(range(0, num_classes))\n",
        "    mapping = dict(zip(y.unique(sorted=False).tolist(), classes))\n",
        "    \n",
        "    # This mapping is necessary when computing the cross-entropy-loss\n",
        "    new_y = torch.tensor(list(map(lambda x: mapping[x], y.tolist())), dtype=y.dtype, device=y.device)\n",
        "    \n",
        "    data_batch = gdata.Data(\n",
        "        x=x, edge_index=edge_index, batch=torch.tensor(batch),\n",
        "        y=new_y, num_graphs=num_graphs, old_classes_mapping=mapping\n",
        "    )\n",
        "\n",
        "    return data_batch\n",
        "\n",
        "\n",
        "def task_sampler_uncollate(task_sampler: 'TaskBatchSampler', data_batch: gdata.Batch):\n",
        "    \"\"\"\n",
        "    Takes as input the task sampler and a batch containing both the \n",
        "    support and the query set. It returns two different DataBatch\n",
        "    respectively for support and query_set.\n",
        "\n",
        "    Assume L = [x1, x2, x3, ..., xN] is the data_batch\n",
        "    each xi is a graph. Moreover, we have that\n",
        "    L[0:K] = support sample for the first class\n",
        "    L[K+1:K+Q] = query sample for the first class\n",
        "    In general, we have that \n",
        "\n",
        "            L[i * (K + Q) : (i + 1) * (K + Q)]\n",
        "\n",
        "    is the (support, query) pair for the i-th class\n",
        "    Finally, the first batch is the one that goes from\n",
        "    L[0 : N * (K + Q)], so\n",
        "\n",
        "            L[i * N * (K + Q) : (i + 1) * N * (K + Q)]\n",
        "\n",
        "    is the i-th batch.\n",
        "\n",
        "    :param task_sampler: The task sampler\n",
        "    :param data_batch: a batch with support and query set\n",
        "    :return: support batch, query batch\n",
        "    \"\"\"\n",
        "    n_way = task_sampler.task_sampler.n_way\n",
        "    k_shot = task_sampler.task_sampler.k_shot\n",
        "    n_query = task_sampler.task_sampler.n_query\n",
        "    task_batch_size = task_sampler.task_batch_size\n",
        "\n",
        "    total_support_query_number = n_way * (k_shot + n_query)\n",
        "    support_plus_query = k_shot + n_query\n",
        "\n",
        "    # Initialize batch list for support and query set\n",
        "    support_data_batch = []\n",
        "    query_data_batch = []\n",
        "\n",
        "    # I know how many batch do I have, so\n",
        "    for batch_number in range(task_batch_size):\n",
        "\n",
        "        # I also know how many class do I have in a task\n",
        "        for class_number in range(n_way):\n",
        "\n",
        "            # First of all let's take the i-th batch\n",
        "            data_batch_slice = slice(\n",
        "                batch_number * total_support_query_number,\n",
        "                (batch_number + 1) * total_support_query_number\n",
        "            )\n",
        "            data_batch_per_batch = data_batch[data_batch_slice]\n",
        "\n",
        "            # Then let's take the (support, query) pair for a class\n",
        "            support_query_slice = slice(\n",
        "                class_number * support_plus_query,\n",
        "                (class_number + 1) * support_plus_query\n",
        "            )\n",
        "            support_query_data = data_batch_per_batch[support_query_slice]\n",
        "\n",
        "            # Divide support from query\n",
        "            support_data = support_query_data[:k_shot]\n",
        "            query_data = support_query_data[k_shot:support_plus_query]\n",
        "\n",
        "            support_data_batch += support_data\n",
        "            query_data_batch += query_data\n",
        "    \n",
        "    # Rename the edges\n",
        "    support_data = data_batch_collate(rename_edge_indexes(support_data_batch))\n",
        "    query_data   = data_batch_collate(rename_edge_indexes(query_data_batch))\n",
        "\n",
        "    # Create new DataBatchs and return\n",
        "    return support_data, query_data"
      ],
      "metadata": {
        "id": "QcbwhI_-X7_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_acc(accs, step, scores, min_step, test_step):\n",
        "    step = np.argmax(scores[min_step - 1 : test_step]) + min_step - 1\n",
        "    return accs[step]\n",
        "\n",
        "\n",
        "def get_batch_number(databatch, i_batch, n_way, k_shot):\n",
        "    \"\"\"From a N batch takes the i-th batch\"\"\"\n",
        "    dim_databatch = n_way * k_shot\n",
        "    indices = torch.arange(0, config.BATCH_PER_EPISODES)\n",
        "    return gdata.Batch.from_data_list(databatch[indices * dim_databatch + i_batch])\n",
        "\n",
        "\n",
        "def glorot(tensor):\n",
        "    \"\"\"Apply the Glorot NN initialization (also called Xavier)\"\"\"\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    \"\"\"Fill a tensor with zeros if it is not Null\"\"\"\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)"
      ],
      "metadata": {
        "id": "lQ10ebDyYOdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset, Sampler and DataLoader"
      ],
      "metadata": {
        "id": "Y1K8uYH0YarQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Dataset\n",
        "\n",
        "I decided to use the same datasets considered in the paper for AS-MAML: TRIANGLES, COIL-DEL, R52 and Letter-High. All of them can be downloaded directly from this [page](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), which is the origin of these datasets. Downloading from the previous page will result in a ZIP file with: \n",
        "\n",
        "- `<dataname>_node_attributes.txt` with the attribute vector for each node of each graph\n",
        "- `<dataname>_graph_labels.txt` with the class for each graph\n",
        "- `<dataname>_graph_edges.txt` with the edges for each graph expressed as a pair (nodex, nodey)\n",
        "- `<dataname>_graph_indicator.txt` that maps each nodes to its corresponding graph\n",
        "\n",
        "Each of the dataset has been splitted into *train*, *test* and *validation*, and transformed into a python dictionaries finally saved as `.pickle` files. In this way we have a ready-to-be-used dataset. Moreover, each ZIP dataset containes three files:\n",
        "\n",
        "- `<dataname>_node_attributes.pickle` with the node attributes saved as a List or a torch Tensor\n",
        "- `<dataname>_train_set.pickle` with all the train data as python dictionaries\n",
        "- `<dataname>_test_set.pickle` with all the test data as python dictionaries\n",
        "- `<dataname>_val_set.pickle` with all the validation data as python dictionaries\n",
        "\n",
        "These are the link from which you can download the datasets: [TRIANGLES](https://drive.google.com/drive/folders/1na8l6DV7qtYIoteFGIp9p7VfQNjmSQxx?usp=sharingwith), [COIL-DEL](https://drive.google.com/drive/folders/1Cq2quq4XNLL91WlwXgXVx3kH_h3_RL9_?usp=sharing), [R52](https://drive.google.com/drive/folders/1pjh1GHn733xb-msqmVP2voZ_IWKKiEYg?usp=sharing) and [Letter-High](\"https://cloud-storage.eu-central-1.linodeobjects.com/Letter-High.zip\")."
      ],
      "metadata": {
        "id": "j4sTfQDU80Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An Example of dataset. In this case the TRIANGLES\n",
        "dataset_name = \"TRIANGLES\"\n",
        "download_folder = os.getcwd()\n",
        "\n",
        "node_attribute, _, train_file, _ = download_zipped_data(\n",
        "    DATASETS[dataset_name], \n",
        "    download_folder, \n",
        "    dataset_name\n",
        ")\n",
        "\n",
        "data_dir = \"/\".join(node_attribute.split(\"/\")[:-2])"
      ],
      "metadata": {
        "id": "CaOA0Fbc86pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Node Attributes Filename --- \", node_attribute)\n",
        "print(\"Train Set Filename --- \", train_file)"
      ],
      "metadata": {
        "id": "jIRXAF_99l_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Node Attribute Content === \")\n",
        "\n",
        "# Convert to torch.Tensor for a pretty printing\n",
        "node_attribute_content = load_with_pickle(node_attribute)\n",
        "if isinstance(list, node_attribute_content):\n",
        "    node_attribute_content = torch.tensor(node_attribute_content)\n",
        "\n",
        "print(node_attribute_content)"
      ],
      "metadata": {
        "id": "9IJzkFoO9_Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as I said the train (or test or validation) set is python dictionary, with three keys: `label2graphs`, mapping each label to a list of corresponding graphs, `graph2nodes`, mapping graphs to their nodes, `graph2edges`, mapping graphs to their egdes (list of nodes pair)."
      ],
      "metadata": {
        "id": "eLqE8Ygp-oaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Train Set Content === \")\n",
        "\n",
        "train_set_content = load_with_pickle(train_file)\n",
        "\n",
        "print(\"Keys --- \", train_set_content.keys())\n",
        "\n",
        "label2graphs = train_set_content[\"label2graphs\"]\n",
        "graph2nodes = train_set_content[\"graph2nodes\"]\n",
        "graph2edges = train_set_content[\"graph2edges\"]\n",
        "\n",
        "print(\"Label2graph example --- \", label2graphs[1])\n",
        "print(\"Graph2nodes example --- \", graph2nodes[1])\n",
        "print(\"Graph2edges example --- \", graph2edges[1])"
      ],
      "metadata": {
        "id": "rdFDWElPOHDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way I handled datasets is different from the one used in the AS-MAML paper. I decided to represent the dataset in python using the class `GraphDataset` that inherit properties and methods from the base class `torch_geometric.data.Dataset`. It is an iterable class and each element (each graph) is of type `torch_geometric.data.Data`. That is, each graph is a `data = Data(x=..., edge_index=..., y=...)`, where `data.x` is a `torch.Tensor` (with dim $\\mathtt{n\\_attribute} \\times 1$) representing the attribute vector of all nodes in the graph, `data.edge_index` is a `torch.Tensor` (with dim $2 \\times \\mathtt{n\\_edges}$) representing the edges of the graph, and finally `data.y` is a `torch.Tensor` (with dim 0) representing the class of that graph."
      ],
      "metadata": {
        "id": "TtJBcpMAPxAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphDataset(gdata.Dataset):\n",
        "    def __init__(self, graphs_ds: Dict[str, Tuple[nx.Graph, str]]) -> None:\n",
        "        super(GraphDataset, self).__init__()\n",
        "        self.graphs_ds = graphs_ds\n",
        "\n",
        "    @classmethod\n",
        "    def get_dataset(cls, attributes: List[Any], data: Dict[str, Any]) -> 'GraphDataset':\n",
        "        \"\"\"\n",
        "        Returns a new instance of GraphDataset filled with graphs inside data. 'attributes'\n",
        "        is the list with all the attributes (not only those beloging to nodes in 'data').\n",
        "\n",
        "        :param data: a dictionary with label2graphs, graph2nodes and graph2edges\n",
        "        :param attributes: a list with node attributes\n",
        "        :return: a new instance of GraphDataset\n",
        "        \"\"\"\n",
        "        graphs = dict()\n",
        "\n",
        "        label2graphs = data[\"label2graphs\"]\n",
        "        graph2nodes  = data[\"graph2nodes\"]\n",
        "        graph2edges  = data[\"graph2edges\"]\n",
        "\n",
        "        for label, graph_list in label2graphs.items():\n",
        "            for graph_id in graph_list:\n",
        "                graph_nodes = graph2nodes[graph_id]\n",
        "                graph_edges = graph2edges[graph_id]\n",
        "                nodes_attributes = [[attributes[node_id - 1]] for node_id in graph_nodes]\n",
        "                nodes = []\n",
        "                for node, attribute in zip(graph_nodes, nodes_attributes):\n",
        "                    nodes.append((node, {f\"attr{i}\" : a for i, a in enumerate(attribute)}))\n",
        "\n",
        "                g = nx.Graph()\n",
        "                g.add_edges_from(graph_edges)\n",
        "                g.add_nodes_from(nodes)\n",
        "            \n",
        "                graphs[graph_id] = (g, label)\n",
        "\n",
        "        graph_dataset = super(GraphDataset, cls).__new__(cls)\n",
        "        graph_dataset.__init__(graphs)\n",
        "\n",
        "        return graph_dataset\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"GraphDataset(classes={set(self.targets().tolist())},n_graphs={self.len()})\"\n",
        "\n",
        "    def indices(self) -> List[str]:\n",
        "        \"\"\" Return all the graph IDs \"\"\"\n",
        "        return list(self.graphs_ds.keys())\n",
        "\n",
        "    def len(self) -> int:\n",
        "        return len(self.graphs_ds.keys())\n",
        "\n",
        "    def targets(self) -> torch.Tensor:\n",
        "        \"\"\" Return all the labels \"\"\"\n",
        "        targets = []\n",
        "        for _, graph in self.graphs_ds.items():\n",
        "            targets.append(int(graph[1]))\n",
        "\n",
        "        return torch.tensor(targets)\n",
        "\n",
        "    def get(self, idx: Union[int, str]) -> gdata.Data:\n",
        "        \"\"\" Return (Graph object, Adjacency matrix and label) of a graph \"\"\"\n",
        "        if isinstance(idx, str):\n",
        "            idx = int(idx)\n",
        "\n",
        "        graph = self.graphs_ds[idx]\n",
        "        g, label = graph[0].to_directed(), graph[1]\n",
        "\n",
        "        # Retrieve nodes attributes\n",
        "        attrs = list(g.nodes(data=True))\n",
        "        x = torch.tensor([list(map(int, a.values())) for _, a in attrs], dtype=torch.float)\n",
        "\n",
        "        # Retrieve edges\n",
        "        edge_index = torch.tensor([list(e) for e in g.edges], dtype=torch.long) \\\n",
        "                          .t()                                                  \\\n",
        "                          .contiguous()\n",
        "\n",
        "        # Retrieve ground trouth labels\n",
        "        y = torch.tensor([int(label)], dtype=torch.int)\n",
        "\n",
        "        return gdata.Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "\n",
        "def generate_train_val_test(dataset_name: str,\n",
        "                            data_dir: Optional[str]=None, \n",
        "                            download: bool=True,\n",
        "                            download_folder: str=\"../data\"\n",
        ") -> Tuple[GraphDataset, GraphDataset, GraphDataset]:\n",
        "    \"\"\" Return dataset for training, validation and testing \"\"\"\n",
        "    logging.debug(\"--- Generating Train, Test and Validation datasets --- \")\n",
        "    \n",
        "    assert download or data_dir is not None, \"At least one between: data_dir and download must be given\"\n",
        "\n",
        "    node_attribute = None\n",
        "    test_file = None\n",
        "    train_file = None\n",
        "    val_file = None\n",
        "\n",
        "    if data_dir is not None:\n",
        "        node_attribute = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_node_attributes.pickle\")\n",
        "        test_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_test_set.pickle\")\n",
        "        train_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_train_set.pickle\")\n",
        "        val_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_val_set.pickle\")\n",
        "\n",
        "    if download:\n",
        "        node_attribute, test_file, train_file, val_file = download_zipped_data(\n",
        "            DATASETS[dataset_name], \n",
        "            download_folder, \n",
        "            dataset_name\n",
        "        )\n",
        "\n",
        "        data_dir = \"/\".join(node_attribute.split(\"/\")[:-2])\n",
        "\n",
        "    node_attribute_data = load_with_pickle(node_attribute)\n",
        "    test_data = load_with_pickle(test_file)\n",
        "    train_data = load_with_pickle(train_file)\n",
        "    val_data = load_with_pickle(val_file)\n",
        "\n",
        "    train_ds = GraphDataset.get_dataset(node_attribute_data, train_data)\n",
        "    test_ds  = GraphDataset.get_dataset(node_attribute_data,  test_data)\n",
        "    val_ds   = GraphDataset.get_dataset(node_attribute_data,   val_data)\n",
        "\n",
        "    return train_ds, test_ds, val_ds, data_dir"
      ],
      "metadata": {
        "id": "FI3PQiGDYaVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Samplers\n",
        "\n",
        "Since we need a specific way to sample from the dataset, in particular the N-way-K-shot (both for support and query set), I encountered the needs of create two samplers: `FewShotSampler` and `TaskBatchSampler`, both inheriting from `torch.utils.data.Sampler`. The former, returns a list of indices indicating which graphs belongs to a single N-way-K-shot sample. The second, just iteratively sampling from `FewShotSampler`, creates mini-batches according to the wanted number of N-way-K-shot sample the user wants in a single batch. The `TaskBatchSampler` is used for the `batch_sampler` argument of the DataLoader."
      ],
      "metadata": {
        "id": "ltfmJDEFTfgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotSampler(torch.utils.data.Sampler):\n",
        "    \"\"\"\n",
        "    In few-shot classification, and in particular in Meta-Learning, \n",
        "    we use a specific way of sampling batches from the training/val/test \n",
        "    set. This way is called N-way-K-shot, where N is the number of classes \n",
        "    to sample per batch and K is the number of examples to sample per class \n",
        "    in the batch. The sample batch on which we train our model is also called \n",
        "    `support` set, while the one on which we test is called `query` set.\n",
        "\n",
        "    This class is a N-way-K-shot sampler that will be used as a batch_sampler\n",
        "    for the :obj:`torch_geometric.loader.DataLoader` dataloader. This sampler\n",
        "    return batches of indices that correspond to support and query set batches.\n",
        "\n",
        "    Attributes:\n",
        "        labels: PyTorch tensor of the labels of the data elements\n",
        "        n_way: Number of classes to sampler per batch\n",
        "        k_shot: Number of examples to sampler per class in the batch\n",
        "        n_query: Number of query example to sample per class in the batch\n",
        "        shuffle: If True, examples and classes are shuffled at each iteration\n",
        "        indices_per_class: How many indices per classes\n",
        "        classes: list of all classes\n",
        "        epoch_size: number of batches per epoch\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels: torch.Tensor,\n",
        "                 n_way: int,\n",
        "                 k_shot: int,\n",
        "                 n_query: int,\n",
        "                 epoch_size: int,\n",
        "                 shuffle: bool = True) -> None:\n",
        "        super().__init__(None)\n",
        "        self.labels = labels\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.n_query = n_query\n",
        "        self.shuffle = shuffle\n",
        "        self.epoch_size = epoch_size\n",
        "\n",
        "        self.classes = torch.unique(self.labels).tolist()\n",
        "        self.indices_per_class = dict()\n",
        "        for cl in self.classes:\n",
        "            self.indices_per_class[cl] = torch.where(self.labels == cl)[0]\n",
        "\n",
        "    def shuffle_data(self) -> None:\n",
        "        \"\"\"\n",
        "        Shuffle the examples per class\n",
        "\n",
        "        Args:\n",
        "            classes: The list of all classes\n",
        "        \"\"\"\n",
        "        for cl in self.classes:\n",
        "            perm = torch.randperm(self.indices_per_class[cl].shape[0])\n",
        "            self.indices_per_class[cl] = self.indices_per_class[cl][perm]\n",
        "\n",
        "    def __iter__(self) -> List[torch.Tensor]:\n",
        "        # Shuffle the data\n",
        "        if self.shuffle:\n",
        "            self.shuffle_data()\n",
        "\n",
        "        target_classes = random.sample(self.classes, self.n_way)\n",
        "        for _ in range(self.epoch_size):\n",
        "            n_way_k_shot_n_query = []\n",
        "            for cl in target_classes:\n",
        "                labels_per_class = self.indices_per_class[cl]\n",
        "                assert len(labels_per_class) >= self.k_shot + self.n_query\n",
        "                selected_data = random.sample(\n",
        "                    labels_per_class.tolist(), self.k_shot + self.n_query)\n",
        "                n_way_k_shot_n_query.append(selected_data)\n",
        "\n",
        "            yield torch.tensor(n_way_k_shot_n_query)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.epoch_size\n",
        "\n",
        "\n",
        "class TaskBatchSampler(torch.utils.data.Sampler):\n",
        "    \"\"\"Sample a batch of tasks\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_targets: torch.Tensor,\n",
        "                       batch_size: int,\n",
        "                       n_way: int,\n",
        "                       k_shot: int,\n",
        "                       n_query: int,\n",
        "                       epoch_size: int,\n",
        "                       shuffle: bool = True) -> None:\n",
        "\n",
        "        super().__init__(None)\n",
        "        self.task_sampler = FewShotSampler(\n",
        "            dataset_targets,\n",
        "            n_way=n_way,\n",
        "            k_shot=k_shot,\n",
        "            n_query=n_query,\n",
        "            epoch_size=epoch_size,\n",
        "            shuffle=shuffle\n",
        "        )\n",
        "\n",
        "        self.task_batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        mini_batches = []\n",
        "        for task_idx, task in enumerate(self.task_sampler):\n",
        "            mini_batches.extend(task.tolist())\n",
        "            if (task_idx + 1) % self.task_batch_size == 0:\n",
        "                yield torch.tensor(mini_batches).flatten().tolist()\n",
        "                mini_batches = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.task_sampler) // self.task_batch_size\n",
        "\n",
        "    def uncollate(self, data_batch):\n",
        "        \"\"\"Invoke the uncollate from utils.utils\"\"\"\n",
        "        return task_sampler_uncollate(self, data_batch)"
      ],
      "metadata": {
        "id": "D6uQ-_BRTibb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The DataLoader\n",
        "\n",
        "In this case each element of the dataset is a `torch_geometric.data.Data` and not just a `torch.Tensor`. For this reason, I decided to create a simple custom dataloader called `FewShotDataLoader` that inherit from `torch.utils.data.DataLoader`. Moreover, there is another problem: `GraphDataset` is not a known type for the default *collate* of PyTorch, or PyTorch-Geometric. So, I created my own collater called `GraphCollater` to manages this situation. "
      ],
      "metadata": {
        "id": "wlu1-zWdWolY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphCollater(gloader.dataloader.Collater):\n",
        "    \"\"\"A Collater to handle batches of GraphDataset instances\"\"\"\n",
        "    def __init__(self, *args) -> None:\n",
        "        super(GraphCollater, self).__init__(*args)\n",
        "\n",
        "    def __call__(self, batch: Generic[T]) -> Generic[T]:\n",
        "        elem = batch[0]\n",
        "\n",
        "        # All elements inside batch is the just a\n",
        "        # repetition of the first element, for this \n",
        "        # reason we can keep just the first one\n",
        "        if isinstance(elem, GraphDataset):\n",
        "            return self(elem)\n",
        "\n",
        "        return super(GraphCollater, self).__call__(batch)\n",
        "\n",
        "\n",
        "class FewShotDataLoader(torch.utils.data.DataLoader):\n",
        "    \"\"\"Custom DataLoader for GraphDataset\"\"\"\n",
        "\n",
        "    def __init__(self, dataset: GraphDataset,\n",
        "                 batch_size: int = 1,\n",
        "                 shuffle: bool = False,\n",
        "                 follow_batch: Optional[List[str]] = None,\n",
        "                 exclude_keys: Optional[List[str]] = None,\n",
        "                 **kwargs) -> None:\n",
        "\n",
        "        if 'collate_fn' in kwargs:\n",
        "            del kwargs[\"collate_fn\"]\n",
        "\n",
        "        self.follow_batch = follow_batch\n",
        "        self.exclude_keys = exclude_keys\n",
        "\n",
        "        # Take the batch sampler\n",
        "        self.batch_sampler = kwargs[\"batch_sampler\"]\n",
        "\n",
        "        super().__init__(\n",
        "            dataset,\n",
        "            batch_size,\n",
        "            shuffle,\n",
        "            collate_fn=GraphCollater(follow_batch, exclude_keys),\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        for x in super().__iter__():\n",
        "            support_batch, query_batch = self.batch_sampler.uncollate(x)\n",
        "            yield support_batch, query_batch"
      ],
      "metadata": {
        "id": "UcPJHNavWq97"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}