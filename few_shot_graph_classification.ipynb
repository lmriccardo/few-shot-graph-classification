{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "few-shot-graph-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Few-Shot Graph Classification\n",
        "\n",
        "Most of the graph classification task overlook the scarcity of labeled graph in many situations. To overcome this problem, *Few-Shot Learning* is started being used. It is a type of Machine Learning method where the training dataset contains limited information. The general practice is to feed the machine learning model with as much data as possible, since this leads to better predictions. However, few-shot learning aims to build accurate machine learning models with less training data. Few-Shot Learning, and in particular in this case Few-shot classification, aims to reduce the cost of gain and label a huge amount of data.\n",
        "\n",
        "*Which is the idea behind Few-Shot Learning*? (on graphs) Given graph data $\\mathcal{G} = \\{(G_1, \\mathbf{y}_1), ..., (G_n, \\mathbf{y}_n)\\}$, we split it into train, $\\{(G^{train}, \\mathbf{y}^{train})\\}$, and test dataset, $\\{(G^{test}, \\mathbf{y}^{test})\\}$. Notice that $\\mathbf{y}^{train}$ and $\\mathbf{y}^{test}$ must have no common classes. For training we use episodic training method, this means that at training stage the algorithm sample a so-called *Task*, i.e., a pair (*support* set, *query* set) where the support set is $D_{sup}^{train} = \\{(G_i^{train}, \\mathbf{y}_{i}^{train})\\}_{i=1}^s$, where $s = N \\times K$, while the query set is $D_{que}^{train} = \\{(G_i^{train}, \\mathbf{y}_{i}^{train})\\}_{i=1}^q$, where $q$ is the number of query data. Given labeled support data, the goal is to predict the labels of query data. Note that in a single task, support data and query data share the same class space. This is also called **N-way-K-shot** learning, where **N** is the number of sampled classes and **K** is the number of samples for each of the N classes. At test stage when performing classification tasks on unseen classes, we firstly fine tune the meta-learner on the support data of test classes, then report classification performance on the test query set.\n",
        "\n",
        "In the following, I'm going to present some approaches in few-shot Learning. First, a *Meta-Learning Framework* based on Fast Weight Adaptation, taken from the paper [Adaptive-Step Graph Meta-Learner for Few-Shot Graph Classification](https://arxiv.org/pdf/2003.08246.pdf) (Ning Ma et al.). Second, I'm going to compare it with different GDA (graph data augmentation) techniques used to enrich the dataset for the novel classes (i.e., those with the less amount of data) taken from a second paper named [Graph Data Augmentation for Graph Machine Learning: A Survey](https://arxiv.org/pdf/2202.08871.pdf) (Tong Zhao et al.)."
      ],
      "metadata": {
        "id": "6YcW8xJl1woa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Modules and Constants"
      ],
      "metadata": {
        "id": "ezvmZRr_RhpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install pyyaml==5.4.1\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "!mkdir models"
      ],
      "metadata": {
        "id": "WGkCr5T9HyrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import (\n",
        "    Any, Dict, List, Tuple, \n",
        "    Union, Generic, Optional,\n",
        "    TypeVar\n",
        ")\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from functools import wraps\n",
        "import plotly.graph_objects as go\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "\n",
        "import torch_geometric.data as gdata\n",
        "import torch_geometric.loader as gloader\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
        "from torch_geometric.nn.inits import uniform\n",
        "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
        "from torch_geometric.nn.pool.sag_pool import SAGPooling\n",
        "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
        "from torch_geometric.utils import (\n",
        "    add_remaining_self_loops, \n",
        "    add_self_loops, \n",
        "    remove_self_loops,\n",
        "    softmax\n",
        ")\n",
        "\n",
        "from torch_scatter import scatter_add\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "WdiE2uITSaS3"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRIANGLES_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/TRIANGLES.zip\"\n",
        "COIL_DEL_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/COIL-DEL.zip\"\n",
        "R52_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/R52.zip\"\n",
        "LETTER_HIGH_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/Letter-High.zip\"\n",
        "\n",
        "DATASETS = {\n",
        "    \"TRIANGLES\"   : TRIANGLES_ZIP_URL, \n",
        "    \"COIL-DEL\"    : COIL_DEL_ZIP_URL, \n",
        "    \"R52\"         : R52_ZIP_URL, \n",
        "    \"Letter-High\" : LETTER_HIGH_ZIP_URL\n",
        "}\n",
        "\n",
        "DEFAULT_DATASET = \"TRIANGLES\"\n",
        "\n",
        "T = TypeVar('T')\n",
        "\n",
        "DEVICE = \"cpu\"\n",
        "DOWNLOAD_DATASET = False\n",
        "SAVE_PICKLE  = True\n",
        "EDGELIMIT_PRINT = 2000\n",
        "SAVE_PRETRAINED = True\n",
        "DATA_PATH = os.path.abspath(os.getcwd()) if not DOWNLOAD_DATASET else None\n",
        "MODELS_SAVE_PATH = \"models\"\n",
        "\n",
        "NUM_FEATURES = {\"TRIANGLES\": 1, \"R52\": 1, \"Letter-High\": 2, \"COIL-DEL\": 2}\n",
        "\n",
        "\n",
        "class ASMAMLConfig:\n",
        "    NHID = 128\n",
        "    POOLING_RATIO = 0.5\n",
        "    DROPOUT_RATIO = 0.3\n",
        "\n",
        "    OUTER_LR     = 0.001\n",
        "    INNER_LR     = 0.01\n",
        "    STOP_LR      = 0.0001\n",
        "    WEIGHT_DECAY = 1E-05\n",
        "\n",
        "    MAX_STEP      = 15\n",
        "    MIN_STEP      = 5\n",
        "    STEP_TEST     = 15\n",
        "    FLEXIBLE_STEP = True\n",
        "    STEP_PENALITY = 0.001\n",
        "    USE_SCORE     = True\n",
        "    USE_GRAD      = False\n",
        "    USE_LOSS      = True\n",
        "\n",
        "    TRAIN_SHOT         = 10   # K-shot for training set\n",
        "    VAL_SHOT           = 10   # K-shot for validation (or test) set\n",
        "    TRAIN_QUERY        = 15   # Number of query for the training set\n",
        "    VAL_QUERY          = 15   # Number of query for the validation (or test) set\n",
        "    TRAIN_WAY          = 3    # N-way for training set\n",
        "    TEST_WAY           = 3    # N-way for test set\n",
        "    VAL_EPISODE        = 200  # Number of episodes for validation\n",
        "    TRAIN_EPISODE      = 200  # Number of episodes for training\n",
        "    BATCH_PER_EPISODES = 5    # How many batch per episode\n",
        "    EPOCHS             = 500  # How many epochs\n",
        "    PATIENCE           = 35\n",
        "    GRAD_CLIP          = 5\n",
        "\n",
        "    # Stop Control configurations\n",
        "    STOP_CONTROL_INPUT_SIZE = 2\n",
        "    STOP_CONTROL_HIDDEN_SIZE = 20"
      ],
      "metadata": {
        "id": "7vboF7-3VcvH"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Utility Functions"
      ],
      "metadata": {
        "id": "qs070YovWwAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scandir(root_path: str) -> List[str]:\n",
        "    \"\"\"Recursively scan a directory looking for files\"\"\"\n",
        "    root_path = os.path.abspath(root_path)\n",
        "    content = []\n",
        "    for file in os.listdir(root_path):\n",
        "        new_path = os.path.join(root_path, file)\n",
        "        if os.path.isfile(new_path):\n",
        "            content.append(new_path)\n",
        "            continue\n",
        "        \n",
        "        content += scandir(new_path)\n",
        "    \n",
        "    return content\n",
        "\n",
        "\n",
        "def download_zipped_data(url: str, path2extract: str, dataset_name: str) -> List[str]:\n",
        "    \"\"\"Download and extract a ZIP file from URL. Return the content filename\"\"\"\n",
        "    print(f\"--- Downloading from {url} ---\")\n",
        "    response = requests.get(url)\n",
        "\n",
        "    abs_path2extract = os.path.abspath(path2extract)\n",
        "    zip_path = os.path.join(abs_path2extract, f\"{dataset_name}.zip\")\n",
        "    with open(zip_path, mode=\"wb\") as iofile:\n",
        "        iofile.write(response.content)\n",
        "\n",
        "    # Extract the file\n",
        "    print(\"--- Extracting files from the archive ---\")\n",
        "    with zipfile.ZipFile(zip_path, mode=\"r\") as zip_ref:\n",
        "        zip_ref.extractall(abs_path2extract)\n",
        "\n",
        "    print(f\"--- Removing {zip_path} ---\")\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    return sorted(scandir(os.path.join(path2extract, dataset_name)))\n",
        "\n",
        "\n",
        "def delete_data_folder(path2delete: str) -> None:\n",
        "    \"\"\"Delete the folder containing data\"\"\"\n",
        "    print(\"--- Removing Content Data ---\")\n",
        "    shutil.rmtree(path2delete)\n",
        "    print(\"--- Removed Finished Succesfully ---\")"
      ],
      "metadata": {
        "id": "hiPgKuQqXDNG"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def elapsed_time(func):\n",
        "    \"\"\"Just a simple wrapper for counting elapsed time from start to end\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(\"Elapsed Time: {:.6f}\".format(end - start))\n",
        "    \n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "7DFVuisvXFHI"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_with_pickle(path2save: str, content: Any) -> None:\n",
        "    \"\"\"Save content inside a .pickle file denoted by path2save\"\"\"\n",
        "    path2save = path2save + \".pickle\" if \".pickle\" not in path2save else path2save\n",
        "    with open(path2save, mode=\"wb\") as iostream:\n",
        "        pickle.dump(content, iostream)\n",
        "\n",
        "\n",
        "def load_with_pickle(path2load: str) -> Any:\n",
        "    \"\"\"Load a content from a .pickle file\"\"\"\n",
        "    with open(path2load, mode=\"rb\") as iostream:\n",
        "        return pickle.load(iostream)"
      ],
      "metadata": {
        "id": "fN6UJb7jXJ_2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "JmagKA3bXL1v"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(G : Union[nx.Graph, nx.DiGraph], name: str) -> None:\n",
        "    \"\"\"\n",
        "    Plot a graph\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    graph : Union[nx.Graph, nx.DiGraph]\n",
        "        Just a nx.Graph object\n",
        "    name  : str\n",
        "        The name of the graph\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Getting the 3D Spring layout\n",
        "    layout = nx.spring_layout(G, dim=3, seed=18)\n",
        "    \n",
        "    # Getting nodes coordinate\n",
        "    x_nodes = [layout[i][0] for i in layout]  # x-coordinates of nodes\n",
        "    y_nodes = [layout[i][1] for i in layout]  # y-coordinates of nodes\n",
        "    z_nodes = [layout[i][2] for i in layout]  # z-coordinates of nodes\n",
        "    \n",
        "    # Getting a list of edges and create a list with coordinates\n",
        "    elist = G.edges()\n",
        "    x_edges, y_edges, z_edges = [], [], []\n",
        "    for edge in elist:\n",
        "        x_edges += [layout[edge[0]][0], layout[edge[1]][0], None]\n",
        "        y_edges += [layout[edge[0]][1], layout[edge[1]][1], None]\n",
        "        z_edges += [layout[edge[0]][2], layout[edge[1]][2], None]\n",
        "\n",
        "    colors = np.linspace(0, len(x_nodes))\n",
        "        \n",
        "    # Create a trace for the edges\n",
        "    etrace = go.Scatter3d(x=x_edges,\n",
        "                          y=y_edges,\n",
        "                          z=z_edges,\n",
        "                          mode='lines',\n",
        "                          line=dict(color='rgb(125,125,125)', width=1),\n",
        "                          hoverinfo='none'\n",
        "                         )\n",
        "    \n",
        "    # Create a trace for the nodes\n",
        "    ntrace = go.Scatter3d(x=x_nodes,\n",
        "                          y=y_nodes,\n",
        "                          z=z_nodes,\n",
        "                          mode='markers',\n",
        "                          marker=dict(\n",
        "                              symbol='circle',\n",
        "                              size=6,\n",
        "                              color=colors,\n",
        "                              colorscale='Viridis',\n",
        "                              line=dict(color='rgb(50,50,50)', width=.5)),\n",
        "                          text=list(layout.keys()),\n",
        "                          hoverinfo='text'\n",
        "                         )\n",
        "    \n",
        "    # Set the axis\n",
        "    axis = dict(showbackground=False,\n",
        "                showline=False,\n",
        "                zeroline=False,\n",
        "                showgrid=False,\n",
        "                showticklabels=False,\n",
        "                title='')\n",
        "    \n",
        "    # Create a layout for the plot\n",
        "    go_layout = go.Layout(title=f\"{name} Network Graph\",\n",
        "                          width=600,\n",
        "                          height=600,\n",
        "                          showlegend=False,\n",
        "                          scene=dict(xaxis=dict(axis),\n",
        "                                     yaxis=dict(axis),\n",
        "                                     zaxis=dict(axis)),\n",
        "                          margin=dict(t=100),\n",
        "                          hovermode='closest'\n",
        "                         )\n",
        "    \n",
        "    # Plot\n",
        "    data = [etrace, ntrace]\n",
        "    fig = go.Figure(data=data, layout=go_layout)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "hA0a8aVGX6kY"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_edge_indexes(data_list: List[gdata.Data]) -> List[gdata.Data]:\n",
        "    \"\"\"\n",
        "    Takes as input a bunch of :obj:`torch_geometric.data.Data` and renames\n",
        "    each edge node (x, y) from 1 to total number of nodes. For instance, if we have\n",
        "    this edge_index = [[1234, 1235, 1236, 1237], [1238, 1239, 1230,1241]] this became\n",
        "    egde_index = [[0, 1, 2, 3],[4, 5, 6, 7]] and so on. \n",
        "\n",
        "    :param data_list: the list of :obj:`torch_geometric.data.Data`\n",
        "    :return: a new list of data\n",
        "    \"\"\"\n",
        "    # First of all let's compute the total number of nodes overall\n",
        "    total_number_nodes = 0\n",
        "    for data in data_list:\n",
        "        total_number_nodes += data.x.shape[0]\n",
        "    \n",
        "    # Generate the new nodes\n",
        "    nodes = torch.arange(0, total_number_nodes)\n",
        "    \n",
        "    # Takes the old nodes from the edge_index attribute\n",
        "    old_nodes = None\n",
        "    for data in data_list:\n",
        "        x, y = data.edge_index\n",
        "        x = torch.hstack((x, y)).unique(sorted=False)\n",
        "        \n",
        "        if old_nodes is None:\n",
        "            old_nodes = x\n",
        "            continue\n",
        "    \n",
        "        old_nodes = torch.hstack((old_nodes, x))\n",
        "    \n",
        "    # Create mapping from old to new nodes\n",
        "    mapping = dict(zip(old_nodes.tolist(), nodes.tolist()[:old_nodes.shape[0]]))\n",
        "    \n",
        "    # Finally, map the new nodes\n",
        "    for data in data_list:\n",
        "        x, y = data.edge_index\n",
        "        new_x = torch.tensor(list(map(lambda x: mapping[x], x.tolist())), dtype=x.dtype, device=x.device)\n",
        "        new_y = torch.tensor(list(map(lambda y: mapping[y], y.tolist())), dtype=y.dtype, device=y.device)\n",
        "        new_edge_index = torch.vstack((new_x, new_y))\n",
        "        data.edge_index = new_edge_index\n",
        "    \n",
        "    return data_list\n",
        "\n",
        "\n",
        "def data_batch_collate(data_list: List[gdata.Data]) -> gdata.Data:\n",
        "    \"\"\"\n",
        "    Takes as input a list of data and create a new :obj:`torch_geometric.data.Data`\n",
        "    collating all together. This is a replacement for torch_geometric.data.Batch.from_data_list\n",
        "\n",
        "    :param data_list: a list of torch_geometric.data.Data objects\n",
        "    :return: a new torch_geometric.data.Data object\n",
        "    \"\"\"\n",
        "    x = None\n",
        "    edge_index = None\n",
        "    batch = []\n",
        "    num_graphs = 0\n",
        "    y = None\n",
        "    \n",
        "    for i_data, data in enumerate(data_list):\n",
        "        x = data.x if x is None else torch.vstack((x, data.x))\n",
        "        edge_index = data.edge_index if edge_index is None else torch.hstack((edge_index, data.edge_index))\n",
        "        batch += [i_data] * data.x.shape[0]\n",
        "        num_graphs += 1\n",
        "        y = data.y if y is None else torch.hstack((y, data.y))\n",
        "\n",
        "    # Create a mapping between y and a range(0, num_classes_of_y)\n",
        "    # First we need to compute how many classes do we have\n",
        "    num_classes = y.unique().shape[0]\n",
        "    classes = list(range(0, num_classes))\n",
        "    mapping = dict(zip(y.unique(sorted=False).tolist(), classes))\n",
        "    \n",
        "    # This mapping is necessary when computing the cross-entropy-loss\n",
        "    new_y = torch.tensor(list(map(lambda x: mapping[x], y.tolist())), dtype=y.dtype, device=y.device)\n",
        "    \n",
        "    data_batch = gdata.Data(\n",
        "        x=x, edge_index=edge_index, batch=torch.tensor(batch),\n",
        "        y=new_y, num_graphs=num_graphs, old_classes_mapping=mapping\n",
        "    )\n",
        "\n",
        "    return data_batch\n",
        "\n",
        "\n",
        "def task_sampler_uncollate(task_sampler: 'TaskBatchSampler', data_batch: gdata.Batch):\n",
        "    \"\"\"\n",
        "    Takes as input the task sampler and a batch containing both the \n",
        "    support and the query set. It returns two different DataBatch\n",
        "    respectively for support and query_set.\n",
        "\n",
        "    Assume L = [x1, x2, x3, ..., xN] is the data_batch\n",
        "    each xi is a graph. Moreover, we have that\n",
        "    L[0:K] = support sample for the first class\n",
        "    L[K+1:K+Q] = query sample for the first class\n",
        "    In general, we have that \n",
        "\n",
        "            L[i * (K + Q) : (i + 1) * (K + Q)]\n",
        "\n",
        "    is the (support, query) pair for the i-th class\n",
        "    Finally, the first batch is the one that goes from\n",
        "    L[0 : N * (K + Q)], so\n",
        "\n",
        "            L[i * N * (K + Q) : (i + 1) * N * (K + Q)]\n",
        "\n",
        "    is the i-th batch.\n",
        "\n",
        "    :param task_sampler: The task sampler\n",
        "    :param data_batch: a batch with support and query set\n",
        "    :return: support batch, query batch\n",
        "    \"\"\"\n",
        "    n_way = task_sampler.task_sampler.n_way\n",
        "    k_shot = task_sampler.task_sampler.k_shot\n",
        "    n_query = task_sampler.task_sampler.n_query\n",
        "    task_batch_size = task_sampler.task_batch_size\n",
        "\n",
        "    total_support_query_number = n_way * (k_shot + n_query)\n",
        "    support_plus_query = k_shot + n_query\n",
        "\n",
        "    # Initialize batch list for support and query set\n",
        "    support_data_batch = []\n",
        "    query_data_batch = []\n",
        "\n",
        "    # I know how many batch do I have, so\n",
        "    for batch_number in range(task_batch_size):\n",
        "\n",
        "        # I also know how many class do I have in a task\n",
        "        for class_number in range(n_way):\n",
        "\n",
        "            # First of all let's take the i-th batch\n",
        "            data_batch_slice = slice(\n",
        "                batch_number * total_support_query_number,\n",
        "                (batch_number + 1) * total_support_query_number\n",
        "            )\n",
        "            data_batch_per_batch = data_batch[data_batch_slice]\n",
        "\n",
        "            # Then let's take the (support, query) pair for a class\n",
        "            support_query_slice = slice(\n",
        "                class_number * support_plus_query,\n",
        "                (class_number + 1) * support_plus_query\n",
        "            )\n",
        "            support_query_data = data_batch_per_batch[support_query_slice]\n",
        "\n",
        "            # Divide support from query\n",
        "            support_data = support_query_data[:k_shot]\n",
        "            query_data = support_query_data[k_shot:support_plus_query]\n",
        "\n",
        "            support_data_batch += support_data\n",
        "            query_data_batch += query_data\n",
        "    \n",
        "    # Rename the edges\n",
        "    support_data = data_batch_collate(rename_edge_indexes(support_data_batch))\n",
        "    query_data   = data_batch_collate(rename_edge_indexes(query_data_batch))\n",
        "\n",
        "    # Create new DataBatchs and return\n",
        "    return support_data, query_data"
      ],
      "metadata": {
        "id": "QcbwhI_-X7_b"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_acc(accs, step, scores, min_step, test_step):\n",
        "    step = np.argmax(scores[min_step - 1 : test_step]) + min_step - 1\n",
        "    return accs[step]\n",
        "\n",
        "\n",
        "def get_batch_number(databatch, i_batch, n_way, k_shot):\n",
        "    \"\"\"From a N batch takes the i-th batch\"\"\"\n",
        "    dim_databatch = n_way * k_shot\n",
        "    indices = torch.arange(0, ASMAMLConfig.BATCH_PER_EPISODES)\n",
        "    return gdata.Batch.from_data_list(databatch[indices * dim_databatch + i_batch])\n",
        "\n",
        "\n",
        "def glorot(tensor):\n",
        "    \"\"\"Apply the Glorot NN initialization (also called Xavier)\"\"\"\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    \"\"\"Fill a tensor with zeros if it is not Null\"\"\"\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)"
      ],
      "metadata": {
        "id": "lQ10ebDyYOdE"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Dataset, Sampler and DataLoader"
      ],
      "metadata": {
        "id": "Y1K8uYH0YarQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Dataset\n",
        "\n",
        "I decided to use the same datasets considered in the paper for AS-MAML: TRIANGLES, COIL-DEL, R52 and Letter-High. All of them can be downloaded directly from this [page](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), which is the origin of these datasets. Downloading from the previous page will result in a ZIP file with: \n",
        "\n",
        "- `<dataname>_node_attributes.txt` with the attribute vector for each node of each graph\n",
        "- `<dataname>_graph_labels.txt` with the class for each graph\n",
        "- `<dataname>_graph_edges.txt` with the edges for each graph expressed as a pair (nodex, nodey)\n",
        "- `<dataname>_graph_indicator.txt` that maps each nodes to its corresponding graph\n",
        "\n",
        "Each of the dataset has been splitted into *train*, *test* and *validation*, and transformed into a python dictionaries finally saved as `.pickle` files. In this way we have a ready-to-be-used dataset. Moreover, each ZIP dataset containes three files:\n",
        "\n",
        "- `<dataname>_node_attributes.pickle` with the node attributes saved as a List or a torch Tensor\n",
        "- `<dataname>_train_set.pickle` with all the train data as python dictionaries\n",
        "- `<dataname>_test_set.pickle` with all the test data as python dictionaries\n",
        "- `<dataname>_val_set.pickle` with all the validation data as python dictionaries\n",
        "\n",
        "These are the link from which you can download the datasets: [TRIANGLES](https://drive.google.com/drive/folders/1na8l6DV7qtYIoteFGIp9p7VfQNjmSQxx?usp=sharingwith), [COIL-DEL](https://drive.google.com/drive/folders/1Cq2quq4XNLL91WlwXgXVx3kH_h3_RL9_?usp=sharing), [R52](https://drive.google.com/drive/folders/1pjh1GHn733xb-msqmVP2voZ_IWKKiEYg?usp=sharing) and [Letter-High](\"https://cloud-storage.eu-central-1.linodeobjects.com/Letter-High.zip\")."
      ],
      "metadata": {
        "id": "j4sTfQDU80Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An Example of dataset. In this case the TRIANGLES\n",
        "dataset_name = \"TRIANGLES\"\n",
        "download_folder = os.getcwd()\n",
        "\n",
        "node_attribute, _, train_file, _ = download_zipped_data(\n",
        "    DATASETS[dataset_name], \n",
        "    download_folder, \n",
        "    dataset_name\n",
        ")\n",
        "\n",
        "data_dir = \"/\".join(node_attribute.split(\"/\")[:-2])"
      ],
      "metadata": {
        "id": "CaOA0Fbc86pN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139eef7c-24f6-4a45-aa8f-33d30adbe291"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Downloading from https://cloud-storage.eu-central-1.linodeobjects.com/TRIANGLES.zip ---\n",
            "--- Extracting files from the archive ---\n",
            "--- Removing /content/TRIANGLES.zip ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Node Attributes Filename --- \", node_attribute)\n",
        "print(\"Train Set Filename --- \", train_file)"
      ],
      "metadata": {
        "id": "jIRXAF_99l_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d2647c-bea1-47cc-c6eb-95f22a874e87"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Attributes Filename ---  /content/TRIANGLES/TRIANGLES_node_attributes.pickle\n",
            "Train Set Filename ---  /content/TRIANGLES/TRIANGLES_train_set.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Node Attribute Content === \")\n",
        "\n",
        "# Convert to torch.Tensor for a pretty printing\n",
        "node_attribute_content = load_with_pickle(node_attribute)\n",
        "if isinstance(node_attribute_content, list):\n",
        "    node_attribute_content = torch.tensor(list(map(int, node_attribute_content))).long()\n",
        "\n",
        "print(node_attribute_content)"
      ],
      "metadata": {
        "id": "9IJzkFoO9_Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eaac0d2-a2f7-4fb9-ca08-156b26bb3f21"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Node Attribute Content === \n",
            "tensor([4, 3, 2,  ..., 2, 3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as I said the train (or test or validation) set is python dictionary, with three keys: `label2graphs`, mapping each label to a list of corresponding graphs, `graph2nodes`, mapping graphs to their nodes, `graph2edges`, mapping graphs to their egdes (list of nodes pair)."
      ],
      "metadata": {
        "id": "eLqE8Ygp-oaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Train Set Content === \")\n",
        "\n",
        "train_set_content = load_with_pickle(train_file)\n",
        "\n",
        "print(\"Keys --- \", train_set_content.keys())\n",
        "\n",
        "label2graphs = train_set_content[\"label2graphs\"]\n",
        "graph2nodes = train_set_content[\"graph2nodes\"]\n",
        "graph2edges = train_set_content[\"graph2edges\"]\n",
        "\n",
        "print(\"Label2graph example --- \", label2graphs[1])\n",
        "print(\"Graph2nodes example --- \", graph2nodes[1])\n",
        "print(\"Graph2edges example --- \", graph2edges[1])"
      ],
      "metadata": {
        "id": "rdFDWElPOHDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a25e936-ecfa-4bd3-f880-f4c16baf04e8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train Set Content === \n",
            "Keys ---  dict_keys(['label2graphs', 'graph2nodes', 'graph2edges'])\n",
            "Label2graph example ---  [0, 1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 44, 45, 46, 47, 48, 49, 51, 53, 54, 56, 57, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 98, 99, 100, 101, 102, 104, 105, 106, 108, 109, 110, 111, 113, 114, 115, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 136, 138, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 200]\n",
            "Graph2nodes example ---  [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
            "Graph2edges example ---  [[21, 22], [21, 23], [21, 24], [21, 25], [26, 27], [26, 25], [22, 21], [22, 23], [22, 28], [22, 29], [23, 21], [23, 22], [23, 30], [23, 31], [23, 32], [28, 22], [28, 32], [24, 21], [24, 27], [24, 32], [27, 26], [27, 24], [27, 29], [27, 31], [29, 22], [29, 27], [29, 32], [25, 21], [25, 26], [25, 30], [30, 23], [30, 25], [31, 23], [31, 27], [32, 23], [32, 28], [32, 24], [32, 29]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way I handled datasets is different from the one used in the AS-MAML paper. I decided to represent the dataset in python using the class `GraphDataset` that inherit properties and methods from the base class `torch_geometric.data.Dataset`. It is an iterable class and each element (each graph) is of type `torch_geometric.data.Data`. That is, each graph is a `data = Data(x=..., edge_index=..., y=...)`, where `data.x` is a `torch.Tensor` (with dim $\\mathtt{n\\_attribute} \\times 1$) representing the attribute vector of all nodes in the graph, `data.edge_index` is a `torch.Tensor` (with dim $2 \\times \\mathtt{n\\_edges}$) representing the edges of the graph, and finally `data.y` is a `torch.Tensor` (with dim 0) representing the class of that graph."
      ],
      "metadata": {
        "id": "TtJBcpMAPxAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphDataset(gdata.Dataset):\n",
        "    def __init__(self, graphs_ds: Dict[str, Tuple[nx.Graph, str]]) -> None:\n",
        "        super(GraphDataset, self).__init__()\n",
        "        self.graphs_ds = graphs_ds\n",
        "\n",
        "    @classmethod\n",
        "    def get_dataset(cls, attributes: List[Any], data: Dict[str, Any]) -> 'GraphDataset':\n",
        "        \"\"\"\n",
        "        Returns a new instance of GraphDataset filled with graphs inside data. 'attributes'\n",
        "        is the list with all the attributes (not only those beloging to nodes in 'data').\n",
        "\n",
        "        :param data: a dictionary with label2graphs, graph2nodes and graph2edges\n",
        "        :param attributes: a list with node attributes\n",
        "        :return: a new instance of GraphDataset\n",
        "        \"\"\"\n",
        "        graphs = dict()\n",
        "\n",
        "        label2graphs = data[\"label2graphs\"]\n",
        "        graph2nodes  = data[\"graph2nodes\"]\n",
        "        graph2edges  = data[\"graph2edges\"]\n",
        "\n",
        "        for label, graph_list in label2graphs.items():\n",
        "            for graph_id in graph_list:\n",
        "                graph_nodes = graph2nodes[graph_id]\n",
        "                graph_edges = graph2edges[graph_id]\n",
        "                nodes_attributes = [[attributes[node_id]] for node_id in graph_nodes]\n",
        "                nodes = []\n",
        "                for node, attribute in zip(graph_nodes, nodes_attributes):\n",
        "                    nodes.append((node, {f\"attr{i}\" : a for i, a in enumerate(attribute)}))\n",
        "\n",
        "                g = nx.Graph()\n",
        "\n",
        "                g.add_edges_from(graph_edges)\n",
        "                g.add_nodes_from(nodes)\n",
        "            \n",
        "                graphs[graph_id] = (g, label)\n",
        "\n",
        "        graphs = dict(sorted(graphs.items(), key=lambda x: x[0]))\n",
        "        graph_dataset = super(GraphDataset, cls).__new__(cls)\n",
        "        graph_dataset.__init__(graphs)\n",
        "\n",
        "        return graph_dataset\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"GraphDataset(classes={set(self.targets().tolist())},n_graphs={self.len()})\"\n",
        "\n",
        "    def indices(self) -> List[str]:\n",
        "        \"\"\" Return all the graph IDs \"\"\"\n",
        "        return list(self.graphs_ds.keys())\n",
        "\n",
        "    def len(self) -> int:\n",
        "        return len(self.graphs_ds.keys())\n",
        "\n",
        "    def targets(self) -> torch.Tensor:\n",
        "        \"\"\" Return all the labels \"\"\"\n",
        "        targets = []\n",
        "        for _, graph in self.graphs_ds.items():\n",
        "            targets.append(int(graph[1]))\n",
        "\n",
        "        return torch.tensor(targets)\n",
        "\n",
        "    def get(self, idx: Union[int, str]) -> gdata.Data:\n",
        "        \"\"\" Return (Graph object, Adjacency matrix and label) of a graph \"\"\"\n",
        "        if isinstance(idx, str):\n",
        "            idx = int(idx)\n",
        "\n",
        "        graph = self.graphs_ds[idx]\n",
        "        g, label = graph[0].to_directed(), graph[1]\n",
        "\n",
        "        # Retrieve nodes attributes\n",
        "        attrs = list(g.nodes(data=True))\n",
        "        x = torch.tensor([list(map(int, a.values())) for _, a in attrs], dtype=torch.float)\n",
        "\n",
        "        # Retrieve edges\n",
        "        edge_index = torch.tensor([list(e) for e in g.edges], dtype=torch.long) \\\n",
        "                          .t()                                                  \\\n",
        "                          .contiguous()                                         \\\n",
        "                          .long()\n",
        "\n",
        "        # Retrieve ground trouth labels\n",
        "        y = torch.tensor([int(label)], dtype=torch.int)\n",
        "\n",
        "        return gdata.Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "\n",
        "def get_all_labels(graphs: Dict[str, Tuple[nx.Graph, str]]) -> torch.Tensor:\n",
        "    \"\"\" Return a list containings all labels of the dataset \"\"\"\n",
        "    return torch.tensor(list(set([int(v[1]) for _, v in graphs.items()])))\n",
        "\n",
        "\n",
        "def generate_train_val_test(dataset_name: str,\n",
        "                            data_dir: Optional[str]=None, \n",
        "                            download: bool=True,\n",
        "                            download_folder: str=\"../data\"\n",
        ") -> Tuple[GraphDataset, GraphDataset, GraphDataset]:\n",
        "    \"\"\" Return dataset for training, validation and testing \"\"\"\n",
        "    print(\"--- Generating Train, Test and Validation datasets --- \")\n",
        "    \n",
        "    assert download or data_dir is not None, \"At least one between: data_dir and download must be given\"\n",
        "\n",
        "    node_attribute = None\n",
        "    test_file = None\n",
        "    train_file = None\n",
        "    val_file = None\n",
        "\n",
        "    if data_dir is not None:\n",
        "        node_attribute = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_node_attributes.pickle\")\n",
        "        test_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_test_set.pickle\")\n",
        "        train_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_train_set.pickle\")\n",
        "        val_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_val_set.pickle\")\n",
        "\n",
        "    if download:\n",
        "        node_attribute, test_file, train_file, val_file = download_zipped_data(\n",
        "            DATASETS[dataset_name], \n",
        "            download_folder, \n",
        "            dataset_name\n",
        "        )\n",
        "\n",
        "        data_dir = \"\\\\\".join(node_attribute.replace(\"\\\\\", \"/\").split(\"/\")[:-2])\n",
        "\n",
        "    node_attribute_data = load_with_pickle(node_attribute)\n",
        "    test_data = load_with_pickle(test_file)\n",
        "    train_data = load_with_pickle(train_file)\n",
        "    val_data = load_with_pickle(val_file)\n",
        "\n",
        "    train_ds = GraphDataset.get_dataset(node_attribute_data, train_data)\n",
        "    test_ds  = GraphDataset.get_dataset(node_attribute_data,  test_data)\n",
        "    val_ds   = GraphDataset.get_dataset(node_attribute_data,   val_data)\n",
        "\n",
        "    return train_ds, test_ds, val_ds, data_dir\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset(download: bool=False, \n",
        "                dataset_name: str=\"TRIANGLES\", \n",
        "                data_dir: str=\"../data\") -> Tuple[GraphDataset, GraphDataset, GraphDataset, str]:\n",
        "    \"\"\"Generate the train, test and validation dataset\"\"\"\n",
        "    data_dir = data_dir if not download else None\n",
        "    train_ds, test_ds, val_ds, data_dir = generate_train_val_test(\n",
        "        data_dir=data_dir,\n",
        "        download=download,\n",
        "        dataset_name=dataset_name\n",
        "    )\n",
        "    return train_ds, test_ds, val_ds, data_dir"
      ],
      "metadata": {
        "id": "FI3PQiGDYaVn"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of dataset\n",
        "train_ds, test_ds, val_ds, _ = get_dataset(data_dir=os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSaEESZw8IC5",
        "outputId": "80c5b533-d09e-4e1c-d98c-9af2f9db240c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Train, Test and Validation datasets --- \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Training Set --- \", train_ds)\n",
        "print(\"--- Test Set --- \", test_ds)\n",
        "print(\"--- Validation Set --- \", val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tB5LV_S9JBG",
        "outputId": "9ef4431e-48f9-44e4-ad71-010fa346c326"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Set ---  GraphDataset(classes={1, 3, 4, 6, 7, 8, 9},n_graphs=1127)\n",
            "--- Test Set ---  GraphDataset(classes={2, 10, 5},n_graphs=603)\n",
            "--- Validation Set ---  GraphDataset(classes={1, 3, 4, 6, 7, 8, 9},n_graphs=280)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Samplers\n",
        "\n",
        "Since we need a specific way to sample from the dataset, in particular the N-way-K-shot (both for support and query set), I encountered the needs of create two samplers: `FewShotSampler` and `TaskBatchSampler`, both inheriting from `torch.utils.data.Sampler`. The former, returns a list of indices indicating which graphs belongs to a single N-way-K-shot sample. The second, just iteratively sampling from `FewShotSampler`, creates mini-batches according to the wanted number of N-way-K-shot sample the user wants in a single batch. The `TaskBatchSampler` is used for the `batch_sampler` argument of the DataLoader."
      ],
      "metadata": {
        "id": "ltfmJDEFTfgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotSampler(torch.utils.data.Sampler):\n",
        "    \"\"\"\n",
        "    In few-shot classification, and in particular in Meta-Learning, \n",
        "    we use a specific way of sampling batches from the training/val/test \n",
        "    set. This way is called N-way-K-shot, where N is the number of classes \n",
        "    to sample per batch and K is the number of examples to sample per class \n",
        "    in the batch. The sample batch on which we train our model is also called \n",
        "    `support` set, while the one on which we test is called `query` set.\n",
        "\n",
        "    This class is a N-way-K-shot sampler that will be used as a batch_sampler\n",
        "    for the :obj:`torch_geometric.loader.DataLoader` dataloader. This sampler\n",
        "    return batches of indices that correspond to support and query set batches.\n",
        "\n",
        "    Attributes:\n",
        "        labels: PyTorch tensor of the labels of the data elements\n",
        "        n_way: Number of classes to sampler per batch\n",
        "        k_shot: Number of examples to sampler per class in the batch\n",
        "        n_query: Number of query example to sample per class in the batch\n",
        "        shuffle: If True, examples and classes are shuffled at each iteration\n",
        "        indices_per_class: How many indices per classes\n",
        "        classes: list of all classes\n",
        "        epoch_size: number of batches per epoch\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels: torch.Tensor,\n",
        "                 n_way: int,\n",
        "                 k_shot: int,\n",
        "                 n_query: int,\n",
        "                 epoch_size: int,\n",
        "                 shuffle: bool = True) -> None:\n",
        "        super().__init__(None)\n",
        "        self.labels = labels\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.n_query = n_query\n",
        "        self.shuffle = shuffle\n",
        "        self.epoch_size = epoch_size\n",
        "\n",
        "        self.classes = torch.unique(self.labels).tolist()\n",
        "        self.indices_per_class = dict()\n",
        "        for cl in self.classes:\n",
        "            self.indices_per_class[cl] = torch.where(self.labels == cl)[0]\n",
        "\n",
        "    def shuffle_data(self) -> None:\n",
        "        \"\"\"\n",
        "        Shuffle the examples per class\n",
        "\n",
        "        Args:\n",
        "            classes: The list of all classes\n",
        "        \"\"\"\n",
        "        for cl in self.classes:\n",
        "            perm = torch.randperm(self.indices_per_class[cl].shape[0])\n",
        "            self.indices_per_class[cl] = self.indices_per_class[cl][perm]\n",
        "\n",
        "    def __iter__(self) -> List[torch.Tensor]:\n",
        "        # Shuffle the data\n",
        "        if self.shuffle:\n",
        "            self.shuffle_data()\n",
        "\n",
        "        target_classes = random.sample(self.classes, self.n_way)\n",
        "        for _ in range(self.epoch_size):\n",
        "            n_way_k_shot_n_query = []\n",
        "            for cl in target_classes:\n",
        "                labels_per_class = self.indices_per_class[cl]\n",
        "                assert len(labels_per_class) >= self.k_shot + self.n_query\n",
        "                selected_data = random.sample(\n",
        "                    labels_per_class.tolist(), self.k_shot + self.n_query)\n",
        "                n_way_k_shot_n_query.append(selected_data)\n",
        "\n",
        "            yield torch.tensor(n_way_k_shot_n_query)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.epoch_size\n",
        "    \n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Return a descriptive string\"\"\"\n",
        "        return \"{name}(classes={cls}, \\n\\t\\t\\tsupport_set_size={sts}, \\n\\t\\t\\tquery_set_size={qts}, \\n\\t\\t\\tsize={size})\".format(\n",
        "            name=self.__class__.__name__, cls=self.classes, sts=f\"{self.n_way} x {self.k_shot}\",\n",
        "            qts=f\"{self.n_way} x {self.n_query}\", size=self.__len__()\n",
        "        )\n",
        "\n",
        "\n",
        "class TaskBatchSampler(torch.utils.data.Sampler):\n",
        "    \"\"\"Sample a batch of tasks\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_targets: torch.Tensor,\n",
        "                 batch_size: int,\n",
        "                 n_way: int,\n",
        "                 k_shot: int,\n",
        "                 n_query: int,\n",
        "                 epoch_size: int,\n",
        "                 shuffle: bool = True) -> None:\n",
        "\n",
        "        super().__init__(None)\n",
        "        self.task_sampler = FewShotSampler(\n",
        "            dataset_targets,\n",
        "            n_way=n_way,\n",
        "            k_shot=k_shot,\n",
        "            n_query=n_query,\n",
        "            epoch_size=epoch_size,\n",
        "            shuffle=shuffle\n",
        "        )\n",
        "\n",
        "        self.task_batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        mini_batches = []\n",
        "        for task_idx, task in enumerate(self.task_sampler):\n",
        "            mini_batches.extend(task.tolist())\n",
        "            if (task_idx + 1) % self.task_batch_size == 0:\n",
        "                yield torch.tensor(mini_batches).flatten().tolist()\n",
        "                mini_batches = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.task_sampler) // self.task_batch_size\n",
        "\n",
        "    def uncollate(self, data_batch):\n",
        "        \"\"\"Invoke the uncollate from utils.utils\"\"\"\n",
        "        return task_sampler_uncollate(self, data_batch)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Return a descriptive string\"\"\"\n",
        "        return \"{name}(task_batch_size={tbs},\\n\\t\\ttask_sampler={ts},\\n\\t\\tsize={size})\".format(\n",
        "            name=self.__class__.__name__, tbs=self.task_batch_size,\n",
        "            ts=self.task_sampler.__repr__(), size=self.__len__()\n",
        "        )\n"
      ],
      "metadata": {
        "id": "D6uQ-_BRTibb"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The DataLoader\n",
        "\n",
        "In this case each element of the dataset is a `torch_geometric.data.Data` and not just a `torch.Tensor`. For this reason, I decided to create a simple custom dataloader called `FewShotDataLoader` that inherit from `torch.utils.data.DataLoader`. Moreover, there is another problem: `GraphDataset` is not a known type for the default *collate* of PyTorch, or PyTorch-Geometric. So, I created my own collater called `GraphCollater` to manages this situation. "
      ],
      "metadata": {
        "id": "wlu1-zWdWolY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphCollater(gloader.dataloader.Collater):\n",
        "    \"\"\"A Collater to handle batches of GraphDataset instances\"\"\"\n",
        "    def __init__(self, *args) -> None:\n",
        "        super(GraphCollater, self).__init__(*args)\n",
        "\n",
        "    def __call__(self, batch: Generic[T]) -> Generic[T]:\n",
        "        elem = batch[0]\n",
        "\n",
        "        # All elements inside batch is the just a\n",
        "        # repetition of the first element, for this \n",
        "        # reason we can keep just the first one\n",
        "        if isinstance(elem, GraphDataset):\n",
        "            return self(elem)\n",
        "\n",
        "        return super(GraphCollater, self).__call__(batch)\n",
        "\n",
        "\n",
        "class FewShotDataLoader(torch.utils.data.DataLoader):\n",
        "    \"\"\"Custom DataLoader for GraphDataset\"\"\"\n",
        "\n",
        "    def __init__(self, dataset: GraphDataset,\n",
        "                 batch_size: int = 1,\n",
        "                 shuffle: bool = False,\n",
        "                 follow_batch: Optional[List[str]] = None,\n",
        "                 exclude_keys: Optional[List[str]] = None,\n",
        "                 **kwargs) -> None:\n",
        "\n",
        "        if 'collate_fn' in kwargs:\n",
        "            del kwargs[\"collate_fn\"]\n",
        "\n",
        "        self.follow_batch = follow_batch\n",
        "        self.exclude_keys = exclude_keys\n",
        "\n",
        "        # Take the batch sampler\n",
        "        self.batch_sampler = kwargs[\"batch_sampler\"]\n",
        "\n",
        "        super().__init__(\n",
        "            dataset,\n",
        "            batch_size,\n",
        "            shuffle,\n",
        "            collate_fn=GraphCollater(follow_batch, exclude_keys),\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        for x in super().__iter__():\n",
        "            support_batch, query_batch = self.batch_sampler.uncollate(x)\n",
        "            yield support_batch, query_batch\n",
        "    \n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Return a descriptive string\"\"\"\n",
        "        return \"{name}(dataset={ds},\\n\\tbatch_sampler={bs},\\n\\tsize={size})\".format(\n",
        "            name=self.__class__.__name__, ds=self.dataset, \n",
        "            bs=self.batch_sampler.__repr__(), size=self.__len__()\n",
        "        )\n",
        "\n",
        "\n",
        "def get_dataloader(\n",
        "    ds: GraphDataset, n_way: int, k_shot: int, n_query: int, \n",
        "    epoch_size: int, shuffle: bool, batch_size: int\n",
        ") -> FewShotDataLoader:\n",
        "    \"\"\"Return a dataloader instance\"\"\"\n",
        "    return FewShotDataLoader(\n",
        "        dataset=ds,\n",
        "        batch_sampler=TaskBatchSampler(\n",
        "            dataset_targets=ds.targets(),\n",
        "            n_way=n_way,\n",
        "            k_shot=k_shot,\n",
        "            n_query=n_query,\n",
        "            epoch_size=epoch_size,\n",
        "            shuffle=shuffle,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "UcPJHNavWq97"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Using the previous generated sets\n",
        "train_dataloader = get_dataloader(\n",
        "    ds=train_ds, n_way=ASMAMLConfig.TRAIN_WAY,\n",
        "    k_shot=ASMAMLConfig.TRAIN_SHOT, n_query=ASMAMLConfig.TRAIN_QUERY,\n",
        "    epoch_size=ASMAMLConfig.TRAIN_EPISODE, shuffle=True, batch_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "rkBjOlZM9Zzg"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Train DataLoader --- \")\n",
        "print(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HILV4YTC-Niy",
        "outputId": "ad9596a4-217b-4148-c20e-35cd3b6b3567"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Train DataLoader --- \n",
            "FewShotDataLoader(dataset=GraphDataset(classes={1, 3, 4, 6, 7, 8, 9},n_graphs=1127),\n",
            "\tbatch_sampler=TaskBatchSampler(task_batch_size=1,\n",
            "\t\ttask_sampler=FewShotSampler(classes=[1, 3, 4, 6, 7, 8, 9], \n",
            "\t\t\tsupport_set_size=3 x 10, \n",
            "\t\t\tquery_set_size=3 x 15, \n",
            "\t\t\tsize=200),\n",
            "\t\tsize=200),\n",
            "\tsize=200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- First Sample ---\")\n",
        "sample = next(iter(train_dataloader))\n",
        "support_data, query_data = sample\n",
        "\n",
        "print(\"--- Support Data ---\")\n",
        "print(support_data)\n",
        "print()\n",
        "\n",
        "print(\"--- Query Data ---\")\n",
        "print(query_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88zpni1n-RHF",
        "outputId": "bba32b75-bb76-49e5-c78b-93d2bb9d2158"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- First Sample ---\n",
            "--- Support Data ---\n",
            "Data(\n",
            "  x=[446, 1],\n",
            "  edge_index=[2, 1542],\n",
            "  y=[30],\n",
            "  batch=[446],\n",
            "  num_graphs=30,\n",
            "  old_classes_mapping={\n",
            "    6=0,\n",
            "    4=1,\n",
            "    9=2\n",
            "  }\n",
            ")\n",
            "\n",
            "--- Query Data ---\n",
            "Data(\n",
            "  x=[731, 1],\n",
            "  edge_index=[2, 2490],\n",
            "  y=[45],\n",
            "  batch=[731],\n",
            "  num_graphs=45,\n",
            "  old_classes_mapping={\n",
            "    6=0,\n",
            "    4=1,\n",
            "    9=2\n",
            "  }\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Models"
      ],
      "metadata": {
        "id": "-dd92mhhzF-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AS-MAML\n",
        "\n",
        "This framework consists of a graph *meta-learner*, which uses GNNs base modules for fast adaptation on graph data and a step controller for robustness and generalization of the meta-learner. They was inspired by the [**Model Agnostic Meta-Learner** (MAML)](https://arxiv.org/pdf/1703.03400.pdf), due to its fast adaptation mechanism. However, directly applying MAML is suboptimal due to the following reasons: painstaking hyperparameter search to reach high generalization; unlike images graphs have arbitrary node size and sub-structure, which brings uncertainty for adaptation. \n",
        "\n",
        "<center>\n",
        "    <img src=\"https://i.imgur.com/SwTvlOE.png\" width=600>\n",
        "</center>"
      ],
      "metadata": {
        "id": "7pMGJsg8zrJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A modification of torch_geometric.nn.conv.gcn_conv.GCNConv\n",
        "# Link: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/gcn_conv.html#GCNConv\n",
        "class GCNConv(MessagePassing):\n",
        "    \"\"\"\n",
        "    GCN Convolutional Layer. \n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
        "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
        "            (default: :obj:`False`)\n",
        "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
        "            the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "            \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n",
        "            cached version for further executions.\n",
        "            This parameter should only be set to :obj:`True` in transductive\n",
        "            learning scenarios. (default: :obj:`False`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int,\n",
        "                       out_channels: int,\n",
        "                       improved: bool=False,\n",
        "                       cached: bool=False,\n",
        "                       bias: bool=True,\n",
        "                       **kwargs):\n",
        "        super().__init__(aggr=\"add\", **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.improved = improved\n",
        "        self.cached = cached\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "        self.weight.fast = None\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "            self.bias.fast = None\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight)\n",
        "        zeros(self.bias)\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
        "        \"\"\"Compute the Norm\"\"\"\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
        "        \n",
        "        fill_value = 1 if not improved else 2\n",
        "        edge_index, edge_weight = add_remaining_self_loops(\n",
        "            edge_index, edge_weight, fill_value, num_nodes\n",
        "        )\n",
        "\n",
        "        row, col = edge_index\n",
        "\n",
        "        # src = edge_weight\n",
        "        # index = row\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "    \n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        \"\"\"The forward method\"\"\"\n",
        "        x = x @ (self.weight if self.weight.fast is None else self.weight.fast)\n",
        "\n",
        "        if self.cached and self.cached_result is not None:\n",
        "            if edge_index.size(1) != self.cached_num_edges:\n",
        "                raise RuntimeError(\n",
        "                    'Cached {} number of edges, but found {}. Please '\n",
        "                    'disable the caching behavior of this layer by removing '\n",
        "                    'the `cached=True` argument in its constructor.'.format(\n",
        "                        self.cached_num_edges, edge_index.size(1)))\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            self.cached_num_edges = edge_index.size(1)\n",
        "            edge_index, norm = self.norm(\n",
        "                edge_index, x.size(self.node_dim), edge_weight,\n",
        "                self.improved, x.dtype\n",
        "            )\n",
        "            self.cached_result = edge_index, norm\n",
        "        \n",
        "        edge_index, norm = self.cached_result\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "    \n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "    \n",
        "    def update(self, aggr_out):\n",
        "        if self.bias is not None:\n",
        "            if self.bias.fast is not None:\n",
        "                aggr_out += self.bias.fast\n",
        "            else:\n",
        "                aggr_out += self.bias\n",
        "        \n",
        "        return aggr_out\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(\n",
        "            self.__class__.__name__, \n",
        "            self.in_channels, \n",
        "            self.out_channels\n",
        "        )\n",
        "\n",
        "\n",
        "# A modification of torch_geometric.nn.conv.sage_conv.SAGEConv\n",
        "# Link: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/sage_conv.html#SAGEConv\n",
        "class SAGEConv(MessagePassing):\n",
        "    \"\"\"\n",
        "    The GraphSAGE operator, modified for the fast weight adaptation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        normalize (bool, optional): If set to :obj:`True`, output features\n",
        "            will be :math:`\\ell_2`-normalized. (default: :obj:`False`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                       normalize: bool=False, bias: bool=True,\n",
        "                       **kwargs) -> None:\n",
        "        super().__init__(aggr='mean', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
        "        self.weight.fast = None\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(self.out_channels))\n",
        "            self.bias.fast = None\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        uniform(self.in_channels, self.weight)\n",
        "        uniform(self.in_channels, self.bias)\n",
        "    \n",
        "    def forward(self, x, edge_index, edge_weight=None, size=None):\n",
        "        if size is None and torch.is_tensor(x):\n",
        "            edge_index, edge_weight = add_remaining_self_loops(\n",
        "                edge_index, edge_weight, 1, x.size(0)\n",
        "            )\n",
        "\n",
        "        if self.weight.fast is not None:\n",
        "            weight = self.weight.fast\n",
        "        else:\n",
        "            weight = self.weight\n",
        "\n",
        "        if torch.is_tensor(x):\n",
        "            x = x @ weight\n",
        "        else:\n",
        "            x0 = None if x[0] is None else x[0] @ weight\n",
        "            x1 = None if x[1] is None else x[1] @ weight\n",
        "            x = (x0, x1)\n",
        "    \n",
        "        return self.propagate(edge_index, size=size, x=x, edge_weight=edge_weight)\n",
        "\n",
        "    def message(self, x_j, edge_weight):\n",
        "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        if self.bias is not None:\n",
        "            if self.bias.fast is not None:\n",
        "                aggr_out = aggr_out + self.bias.fast\n",
        "            else:\n",
        "                aggr_out = aggr_out + self.bias\n",
        "        if self.normalize:\n",
        "            aggr_out = F.normalize(aggr_out, p=2, dim=-1)\n",
        "        return aggr_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)\n",
        "\n",
        "\n",
        "# A modification of torch_geometric.nn.conv.graph_conv.GraphConv\n",
        "# Link: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/graph_conv.html#GraphConv\n",
        "class GraphConv(MessagePassing):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        aggr (string, optional): The aggregation scheme to use\n",
        "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
        "            (default: :obj:`\"add\"`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, aggr='add', bias=True,\n",
        "                 **kwargs):\n",
        "        super(GraphConv, self).__init__(aggr=aggr, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "        self.lin = LinearModel(in_channels, out_channels)\n",
        "        self.weight.fast = None\n",
        "        self.lin.weight.fast = None\n",
        "        self.lin.bias.fast = None\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        uniform(self.in_channels, self.weight)\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None, size=None):\n",
        "        \"\"\"\"\"\"\n",
        "        if self.weight.fast is not None:\n",
        "            h = x @ self.weight.fast\n",
        "        else:\n",
        "            h = x @ self.weight\n",
        "\n",
        "        return self.propagate(edge_index, size=size, x=x, h=h,\n",
        "                              edge_weight=edge_weight)\n",
        "\n",
        "    def message(self, h_j, edge_weight):\n",
        "        return h_j if edge_weight is None else edge_weight.view(-1, 1) * h_j\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        return aggr_out + self.lin(x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)"
      ],
      "metadata": {
        "id": "kN2GxI7RzL3f"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearModel(nn.Linear):\n",
        "    \"\"\"A Simple Linear model implementation for fast weights\"\"\"\n",
        "    def __init__(self, in_features: int, out_features: int) -> None:\n",
        "        super().__init__(in_features, out_features, bias=True)\n",
        "        self.weight.fast = None\n",
        "        self.bias.fast = None\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.weight.fast is not None and self.bias.fast is not None:\n",
        "            return F.linear(x, self.weight.fast, self.bias.fast)\n",
        "        return super().forward(x)"
      ],
      "metadata": {
        "id": "ZwHory5YCOHj"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A Modification of torch_geometric.nn.pool.topk_pool.TopKPool\n",
        "# Link: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/pool/topk_pool.html#TopKPooling\n",
        "class TopKPooling(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        ratio (float): Graph pooling ratio, which is used to compute\n",
        "            :math:`k = \\lceil \\mathrm{ratio} \\cdot N \\rceil`.\n",
        "            This value is ignored if min_score is not None.\n",
        "            (default: :obj:`0.5`)\n",
        "        min_score (float, optional): Minimal node score :math:`\\tilde{\\alpha}`\n",
        "            which is used to compute indices of pooled nodes\n",
        "            :math:`\\mathbf{i} = \\mathbf{y}_i > \\tilde{\\alpha}`.\n",
        "            When this value is not :obj:`None`, the :obj:`ratio` argument is\n",
        "            ignored. (default: :obj:`None`)\n",
        "        multiplier (float, optional): Coefficient by which features gets\n",
        "            multiplied after pooling. This can be useful for large graphs and\n",
        "            when :obj:`min_score` is used. (default: :obj:`1`)\n",
        "        nonlinearity (torch.nn.functional, optional): The nonlinearity to use.\n",
        "            (default: :obj:`torch.tanh`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, \n",
        "                       ratio: float=0.5,\n",
        "                       min_score: Optional[float]=None,\n",
        "                       multiplier: int=1,\n",
        "                       nonlinearity=torch.tanh) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "        self.ratio = ratio\n",
        "        self.min_score = min_score\n",
        "        self.multiplier = multiplier\n",
        "        self.nonlinearity = nonlinearity\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(1, in_channels))\n",
        "        self.weight.fast = None\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reset the parameters\"\"\"\n",
        "        size = self.in_channels\n",
        "        uniform(size, self.weight)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None, batch=None, attn=None):\n",
        "        \"\"\"The forward method\"\"\"\n",
        "        if batch is None:\n",
        "            batch = edge_index.new_zeros(x.size(0))\n",
        "\n",
        "        attn = x if attn is None else attn\n",
        "        attn = attn.unsqueeze(-1) if attn.dim() == 1 else attn\n",
        "\n",
        "        if self.weight.fast is not None:\n",
        "            score = (attn * self.weight.fast).sum(dim=-1)\n",
        "        else:\n",
        "            score = (attn * self.weight).sum(dim=-1)\n",
        "        \n",
        "        if self.min_score is None:\n",
        "            if self.weight.fast is not None:\n",
        "                score = self.nonlinearity(score / self.weight.fast.norm(p=2, dim=-1))\n",
        "            else:\n",
        "                score = self.nonlinearity(score / self.weight.norm(p=2, dim=-1))\n",
        "        else:\n",
        "            score = softmax(score, batch)\n",
        "        \n",
        "        perm = topk(score, self.ratio, batch, self.min_score)\n",
        "        x = x[perm] * score[perm].view(-1, 1)\n",
        "        x = self.multiplier * x if self.multiplier != 1 else x\n",
        "\n",
        "        batch = batch[perm]\n",
        "        edge_index, edge_attr = filter_adj(\n",
        "            edge_index, edge_attr, perm,\n",
        "            num_nodes=score.size(0)\n",
        "        )\n",
        "\n",
        "        return x, edge_index, edge_attr, batch, perm, score[perm]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}={}, multiplier={})'.format(\n",
        "            self.__class__.__name__, self.in_channels,\n",
        "            'ratio' if self.min_score is None else 'min_score',\n",
        "            self.ratio if self.min_score is None else self.min_score,\n",
        "            self.multiplier)\n",
        "        \n",
        "\n",
        "# A very simple modification of torch_geometric.nn.pool.sag_pool.SAGPooling\n",
        "# Link: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/pool/sag_pool.html#SAGPooling\n",
        "class SAGPool4MAML(SAGPooling):\n",
        "    \"\"\"SAGPooling for MAML. Change only the __repr__ method\"\"\"\n",
        "    def __init__(self, in_channels: int, ratio: float=0.5,\n",
        "                       GNN: nn.Module=GraphConv, min_score: Optional[float]=None,\n",
        "                       multiplier: int=1, nonlinearity=torch.tanh, **kwargs) -> None:\n",
        "        super().__init__(\n",
        "            in_channels=in_channels, ratio=ratio,\n",
        "            GNN=GNN, min_score=min_score, multiplier=multiplier,\n",
        "            nonlinearity=nonlinearity, **kwargs\n",
        "        )\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return '{}({}, {}, {}={}, multiplier={})'.format(\n",
        "            self.__class__.__name__, self.gnn.__class__.__name__,\n",
        "            self.in_channels,\n",
        "            'ratio' if self.min_score is None else 'min_score',\n",
        "            self.ratio if self.min_score is None else self.min_score,\n",
        "            self.multiplier)"
      ],
      "metadata": {
        "id": "tlLA5LPjCqra"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StopControl(nn.Module):\n",
        "    \"\"\"For computing the stop probability\"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super(StopControl, self).__init__()\n",
        "        self.lstm = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "        self.output_layer.bias.data.fill_(0.0)\n",
        "        self.h_0 = nn.Parameter(torch.randn((hidden_size, ), requires_grad=True))\n",
        "        self.c_0 = nn.Parameter(torch.randn((hidden_size, ), requires_grad=True))\n",
        "\n",
        "    def forward(self, inputs, hx) -> torch.Tensor:\n",
        "        if hx is None:\n",
        "            hx = (self.h_0.unsqueeze(0), self.c_0.unsqueeze(0))\n",
        "        \n",
        "        h, c = self.lstm(inputs, hx)\n",
        "        return torch.sigmoid(self.output_layer(h).unsqueeze(0)), (h, c)"
      ],
      "metadata": {
        "id": "h7irRlyIDK8V"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NodeInformationScore(MessagePassing):\n",
        "    \"\"\"Node information score\"\"\"\n",
        "    def __init__(self, improved=False, cached=False, **kwargs):\n",
        "        super().__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.improved = improved\n",
        "        self.cached = cached\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "    \n",
        "    @staticmethod\n",
        "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
        "        \n",
        "        row, col = edge_index\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        edge_index, edge_weight = add_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
        "\n",
        "        row, col = edge_index\n",
        "        expand_deg = torch.zeros((edge_weight.size(0), ), dtype=dtype, device=edge_index.device)\n",
        "        expand_deg[-num_nodes:] = torch.ones((num_nodes, ), dtype=dtype, device=edge_index.device)\n",
        "\n",
        "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        if self.cached and self.cached_result is not None:\n",
        "            if edge_index.size(1) != self.cached_num_edges:\n",
        "                raise RuntimeError(\n",
        "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            self.cached_num_edges = edge_index.size(1)\n",
        "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
        "            self.cached_result = edge_index, norm\n",
        "\n",
        "        edge_index, norm = self.cached_result\n",
        "\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "\n",
        "class GCN4MAML(nn.Module):\n",
        "    \"\"\"GCN for AS-MAML\"\"\"\n",
        "    def __init__(self, num_features: int=1, num_classes: int=30) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.num_classes  = num_classes\n",
        "        \n",
        "        # Define convolutional layers\n",
        "        self.conv1 = GCNConv(self.num_features, ASMAMLConfig.NHID)\n",
        "        self.conv2 = GCNConv(ASMAMLConfig.NHID, ASMAMLConfig.NHID)\n",
        "        self.conv3 = GCNConv(ASMAMLConfig.NHID, ASMAMLConfig.NHID)\n",
        "\n",
        "        self.calc_information_score = NodeInformationScore()\n",
        "\n",
        "        # Define Pooling layers\n",
        "        self.pool1 = TopKPooling(ASMAMLConfig.NHID, ASMAMLConfig.POOLING_RATIO)\n",
        "        self.pool2 = TopKPooling(ASMAMLConfig.NHID, ASMAMLConfig.POOLING_RATIO)\n",
        "        self.pool3 = TopKPooling(ASMAMLConfig.NHID, ASMAMLConfig.POOLING_RATIO)\n",
        "\n",
        "        # Define Linear Layers\n",
        "        self.linear1 = LinearModel(ASMAMLConfig.NHID * 2, ASMAMLConfig.NHID)\n",
        "        self.linear2 = LinearModel(ASMAMLConfig.NHID, ASMAMLConfig.NHID // 2)\n",
        "        self.linear3 = LinearModel(ASMAMLConfig.NHID // 2, self.num_classes)\n",
        "\n",
        "        # Define activation function\n",
        "        self.relu = F.leaky_relu\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        edge_attr = None\n",
        "\n",
        "        x = self.relu(self.conv1(x, edge_index, edge_attr), negative_slope=0.1)\n",
        "        x, edge_index, edge_attr, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
        "        x1 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
        "\n",
        "        x = self.relu(self.conv2(x, edge_index, edge_attr), negative_slope=0.1)\n",
        "        x, edge_index, edge_attr, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
        "        x2 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
        "\n",
        "        x = self.relu(self.conv3(x, edge_index, edge_attr), negative_slope=0.1)\n",
        "        x, edge_index, edge_attr, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
        "\n",
        "        x_information_score = self.calc_information_score(x, edge_index)\n",
        "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
        "        x3 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
        "\n",
        "        x = self.relu(x1, negative_slope=0.1) + \\\n",
        "            self.relu(x2, negative_slope=0.1) + \\\n",
        "            self.relu(x3, negative_slope=0.1)\n",
        "        \n",
        "        x = self.relu(self.linear1(x), negative_slope=0.1)\n",
        "        x = self.relu(self.linear2(x), negative_slope=0.1)\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x, score.mean(), None\n",
        "\n",
        "\n",
        "class SAGE4MAML(nn.Module):\n",
        "    \"\"\"SAGE Model 4 MAML\"\"\"\n",
        "    def __init__(self, num_features: int=1, num_classes: int=30) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.num_classes  = num_classes\n",
        "        \n",
        "        # Define convolutional layers\n",
        "        self.conv1 = SAGEConv(self.num_features, ASMAMLConfig.NHID)\n",
        "        self.conv2 = SAGEConv(ASMAMLConfig.NHID, ASMAMLConfig.NHID)\n",
        "        self.conv3 = SAGEConv(ASMAMLConfig.NHID, ASMAMLConfig.NHID)\n",
        "\n",
        "        self.calc_information_score = NodeInformationScore()\n",
        "\n",
        "        # Define Pooling layers\n",
        "        self.pool1 = SAGPool4MAML(ASMAMLConfig.NHID, ASMAMLConfig.POOLING_RATIO)\n",
        "        self.pool2 = SAGPool4MAML(ASMAMLConfig.NHID, ASMAMLConfig.POOLING_RATIO)\n",
        "        self.pool3 = SAGPool4MAML(ASMAMLConfig.NHID, ASMAMLConfig.POOLING_RATIO)\n",
        "\n",
        "        # Define Linear Layers\n",
        "        self.linear1 = LinearModel(ASMAMLConfig.NHID * 2, ASMAMLConfig.NHID)\n",
        "        self.linear2 = LinearModel(ASMAMLConfig.NHID, ASMAMLConfig.NHID // 2)\n",
        "        self.linear3 = LinearModel(ASMAMLConfig.NHID // 2, self.num_classes)\n",
        "\n",
        "        # Define activation function\n",
        "        self.relu = F.leaky_relu\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        edge_attr = None\n",
        "\n",
        "        x = self.relu(self.conv1(x, edge_index, edge_attr),negative_slope=0.1)\n",
        "        x, edge_index, edge_attr, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
        "        x1 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
        "\n",
        "        x =self.relu(self.conv2(x, edge_index, edge_attr),negative_slope=0.1)\n",
        "        x, edge_index, edge_attr, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
        "        x2 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
        "\n",
        "        x = self.relu(self.conv3(x, edge_index, edge_attr), negative_slope=0.1)\n",
        "        x, edge_index, edge_attr, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
        "        x3 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
        "\n",
        "        x_information_score = self.calc_information_score(x, edge_index)\n",
        "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
        "\n",
        "        x = self.relu(x1,negative_slope=0.1) + \\\n",
        "            self.relu(x2,negative_slope=0.1) + \\\n",
        "            self.relu(x3,negative_slope=0.1)\n",
        "\n",
        "        graph_emb = x\n",
        "\n",
        "        x = self.relu(self.linear1(x),negative_slope=0.1)\n",
        "        x = self.relu(self.linear2(x),negative_slope=0.1)\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x, score.mean(), graph_emb"
      ],
      "metadata": {
        "id": "Ywn-3yGPDbg8"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaptiveStepMAML Class Here\n",
        "class AdaptiveStepMAML(nn.Module):\n",
        "    \"\"\" The Meta-Learner Class \"\"\"\n",
        "    def __init__(self, model: Union[GCN4MAML, SAGE4MAML], inner_lr: float, \n",
        "                 outer_lr: float, stop_lr: float, weight_decay: float) -> None:\n",
        "        super().__init__()\n",
        "        self.net          = model\n",
        "        self.inner_lr     = inner_lr\n",
        "        self.outer_lr     = outer_lr\n",
        "        self.stop_lr      = stop_lr\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        self.task_index = 1\n",
        "        self.stop_prob = 0.5\n",
        "        self.stop_gate = StopControl(ASMAMLConfig.STOP_CONTROL_INPUT_SIZE, ASMAMLConfig.STOP_CONTROL_HIDDEN_SIZE)\n",
        "\n",
        "        self.meta_optim = self.configure_optimizers()\n",
        "\n",
        "        self.loss      = nn.CrossEntropyLoss()\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.meta_optim, mode=\"min\", factor=0.5, \n",
        "            patience=ASMAMLConfig.PATIENCE, verbose=True, min_lr=1e-05\n",
        "        )\n",
        "\n",
        "        self.graph_embs = []\n",
        "        self.graph_labels = []\n",
        "        self.index = 1\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Configure Optimizers\"\"\"\n",
        "        return optim.Adam([\n",
        "                           {'params': self.net.parameters(),       'lr': self.outer_lr},\n",
        "                           {'params': self.stop_gate.parameters(), 'lr': self.stop_lr}],\n",
        "                          lr=1e-04, weight_decay=self.weight_decay\n",
        "               )\n",
        "        \n",
        "    def compute_loss(self, logits: torch.Tensor, label: torch.Tensor) -> float:\n",
        "        \"\"\"Compute the loss\"\"\"\n",
        "        return self.loss(logits, label.long())\n",
        "\n",
        "    @staticmethod\n",
        "    def smooth(weight, p=10, eps=1e-10):\n",
        "        weight_abs = weight.abs()\n",
        "        less = (weight_abs < math.exp(-p)).type(torch.float)\n",
        "        noless = 1.0 - less\n",
        "        log_weight = less * -1 + noless * torch.log(weight_abs + eps) / p\n",
        "        sign = less * math.exp(p) * weight + noless * weight.sign()\n",
        "        assert  torch.sum(torch.isnan(log_weight))==0,'stop_gate input has nan'\n",
        "        return log_weight, sign\n",
        "\n",
        "    def stop(self, step: int, loss: float, node_score: torch.Tensor):\n",
        "        stop_hx = None\n",
        "        if ASMAMLConfig.FLEXIBLE_STEP and step < ASMAMLConfig.MAX_STEP:\n",
        "            inputs = []\n",
        "\n",
        "            if ASMAMLConfig.USE_LOSS:\n",
        "                inputs += [loss.detach()]\n",
        "            if ASMAMLConfig.USE_SCORE:\n",
        "                score = node_score.detach()\n",
        "                inputs += [score]\n",
        "\n",
        "            inputs = torch.stack(inputs, dim=0).unsqueeze(0)\n",
        "            inputs = self.smooth(inputs)[0]\n",
        "            stop_gate, stop_hx = self.stop_gate(inputs, stop_hx)\n",
        "\n",
        "            return stop_gate    \n",
        "\n",
        "        return loss.new_zeros(1, dtype=torch.float)\n",
        "\n",
        "    def adapt_meta_learning_rate(self, loss):\n",
        "        self.scheduler.step(loss)\n",
        "    \n",
        "    def get_meta_learning_rate(self):\n",
        "        epoch_learning_rate = []\n",
        "        for param_group in self.meta_optim.param_groups:\n",
        "            epoch_learning_rate.append(param_group['lr'])\n",
        "        return epoch_learning_rate[0]\n",
        "\n",
        "    def forward(self, support_data: gdata.batch.Batch, query_data: gdata.batch.Batch):\n",
        "        # It is just the number of labels to predict in the query set\n",
        "        query_size = query_data.y.shape[0]\n",
        "\n",
        "        losses_q = []  # Losses on query data\n",
        "        corrects, stop_gates, train_losses, train_accs, scores = [], [], [], [], []\n",
        "        \n",
        "        fast_parameters = list(self.net.parameters())\n",
        "\n",
        "        for weight in self.net.parameters():\n",
        "            weight.fast = None\n",
        "        \n",
        "        step = 0\n",
        "        self.stop_prob = 0.1 if self.stop_prob < 0.1 else self.stop_prob\n",
        "\n",
        "        # Get adaptation step\n",
        "        ada_step = min(ASMAMLConfig.MAX_STEP, ASMAMLConfig.MIN_STEP + int(1.0 / self.stop_prob))\n",
        "\n",
        "        for k in range(0, ada_step):\n",
        "            # Run the i-th task and compute the loss\n",
        "            logits, score, _ = self.net(support_data.x, support_data.edge_index, support_data.batch)\n",
        "            loss = self.compute_loss(logits, support_data.y)\n",
        "\n",
        "            stop_probability = 0\n",
        "            if ASMAMLConfig.FLEXIBLE_STEP:\n",
        "                stop_probability = self.stop(k, loss, score)\n",
        "                self.stop_prob = stop_probability\n",
        "            \n",
        "            stop_gates.append(stop_probability)\n",
        "            scores.append(score.item())\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = F.softmax(logits, dim=1).argmax(dim=1)\n",
        "                correct = torch.eq(pred, support_data.y).sum().item()\n",
        "                train_accs.append(correct / support_data.y.shape[0])\n",
        "\n",
        "            step = k\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Compute the gradient with respect to the loss\n",
        "            grad = torch.autograd.grad(loss, fast_parameters, create_graph=True)\n",
        "            fast_parameters = []\n",
        "            for index, weight in enumerate(self.net.parameters()):\n",
        "                if weight.fast is not None:\n",
        "                    weight.fast = weight.fast - self.inner_lr * grad[index]\n",
        "                else:\n",
        "                    weight.fast = weight - self.inner_lr * grad[index]\n",
        "                \n",
        "                fast_parameters.append(weight.fast)\n",
        "            \n",
        "            logits_q, _, _ = self.net(query_data.x, query_data.edge_index, query_data.batch)\n",
        "            loss_q = self.compute_loss(logits_q, query_data.y)\n",
        "\n",
        "            losses_q.append(loss_q)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
        "                correct = torch.eq(pred_q, query_data.y).sum().item()\n",
        "                corrects.append(correct)\n",
        "        \n",
        "        final_loss = losses_q[step]\n",
        "        accs = np.array(corrects) / (query_size)\n",
        "        final_acc = accs[step]\n",
        "        total_loss = 0\n",
        "\n",
        "        if ASMAMLConfig.FLEXIBLE_STEP:\n",
        "            for step, (stop_gate, step_acc) in enumerate(zip(stop_gates[ASMAMLConfig.MIN_STEP - 1:], accs[ASMAMLConfig.MIN_STEP - 1:])):\n",
        "                assert stop_gate >= 0.0 and stop_gate <= 1.0, \"stop_gate error value: {:.5f}\".format(stop_gate)\n",
        "                log_prob = torch.log(1 - stop_gate)\n",
        "                tem_loss = - log_prob * ((final_acc - step_acc - (np.exp(step) - 1) * ASMAMLConfig.STEP_PENALITY))\n",
        "                total_loss += tem_loss\n",
        "\n",
        "            total_loss = (total_loss + final_acc + final_loss)\n",
        "        else:\n",
        "            total_loss = final_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "\n",
        "        if self.task_index == ASMAMLConfig.BATCH_PER_EPISODES:\n",
        "            if ASMAMLConfig.GRAD_CLIP > 0.1:\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), ASMAMLConfig.GRAD_CLIP)\n",
        "\n",
        "            self.meta_optim.step()\n",
        "            self.meta_optim.zero_grad()\n",
        "            self.task_index = 1\n",
        "        else:\n",
        "            self.task_index += 1\n",
        "        \n",
        "        if ASMAMLConfig.FLEXIBLE_STEP:\n",
        "            stop_gates = [stop_gate.item() for stop_gate in stop_gates]\n",
        "\n",
        "        return accs * 100, step, final_loss.item(), total_loss.item(), stop_gates, scores, train_losses, train_accs\n",
        "\n",
        "    def finetuning(self, support_data, query_data):\n",
        "        # It is just the number of labels to predict in the query set\n",
        "        query_size = query_data.y.shape[0]\n",
        "\n",
        "        corrects = []\n",
        "        step = 0\n",
        "        stop_gates, scores, query_loss = [], [], []\n",
        "\n",
        "        fast_parameters = list(self.net.parameters())\n",
        "\n",
        "        for weight in self.net.parameters():\n",
        "            weight.fast = None\n",
        "        \n",
        "        ada_step = min(ASMAMLConfig.STEP_TEST, ASMAMLConfig.MIN_STEP + int(2 / self.stop_prob))\n",
        "\n",
        "        for k in range(ada_step):\n",
        "            logits, score, _ = self.net(support_data.x, support_data.edge_index, support_data.batch)\n",
        "            loss = self.compute_loss(logits, support_data.y)\n",
        "\n",
        "            stop_probability = 0\n",
        "\n",
        "            if ASMAMLConfig.FLEXIBLE_STEP:\n",
        "                with torch.no_grad():\n",
        "                    stop_probability = self.stop(k, loss, score)\n",
        "            \n",
        "            stop_gates.append(stop_probability)\n",
        "            step = k\n",
        "            scores.append(score.item())\n",
        "\n",
        "            grad = torch.autograd.grad(loss, fast_parameters, create_graph=True)\n",
        "            fast_parameters = []\n",
        "\n",
        "            for index, weight in enumerate(self.net.parameters()):\n",
        "                if weight.fast is None:\n",
        "                    weight.fast = weight - ASMAMLConfig.INNER_LR * grad[index]\n",
        "                else:\n",
        "                    weight.fast = weight.fast - ASMAMLConfig.INNER_LR * grad[index]\n",
        "\n",
        "                fast_parameters.append(weight.fast)\n",
        "\n",
        "            logits_q, _, graph_emb = self.net(query_data.x, query_data.edge_index, query_data.batch)\n",
        "            self.graph_labels.append(query_data.y)\n",
        "\n",
        "            self.graph_embs.append(graph_emb)\n",
        "\n",
        "            if self.index % 1 == 0:\n",
        "                self.index = 1\n",
        "                self.graph_embs = []\n",
        "                self.graph_labels = []\n",
        "            else:\n",
        "                self.index += 1\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
        "                correct = torch.eq(pred_q, query_data.y).sum().item()\n",
        "                corrects.append(correct)\n",
        "                \n",
        "                loss_query = self.compute_loss(logits_q, query_data.y)\n",
        "                query_loss.append(loss_query.item())\n",
        "\n",
        "        accs = 100 * np.array(corrects) / query_size\n",
        "\n",
        "        if ASMAMLConfig.FLEXIBLE_STEP:\n",
        "            stop_gates = [stop_gate.item() for stop_gate in stop_gates]\n",
        "\n",
        "        return accs, step, stop_gates, scores, query_loss"
      ],
      "metadata": {
        "id": "mtwBsmRqEgxr"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Training and Testing"
      ],
      "metadata": {
        "id": "KJ9o_GrbC1k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First of all let's define two python class for training (optimization) and testing\n",
        "class Optimizer:\n",
        "    \"\"\"\n",
        "    Run Training with train set and validation set\n",
        "\n",
        "    Attributes:\n",
        "        train_ds (GraphDataset): the train set\n",
        "        val_ds (GraphDataset): the validation set\n",
        "        model_name (str, default=\"sage\"): the name of the model to use ('sage' or 'gcn')\n",
        "        epochs (int, default=200): number of epochs to run\n",
        "        dataset_name (str, default=\"TRIANGLES\"): the name of the dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, train_ds: GraphDataset, val_ds: GraphDataset, \n",
        "                 model_name: str=\"sage\", epochs: int=200, \n",
        "                 dataset_name: str=\"TRIANGLES\", config_class: object=ASMAMLConfig\n",
        "    ) -> None:\n",
        "        self.train_ds = train_ds\n",
        "        self.val_ds = val_ds\n",
        "        self.model_name = model_name\n",
        "        self.epochs = epochs\n",
        "        self.dataset_name = dataset_name\n",
        "        self.config = config_class\n",
        "\n",
        "        self.model = self.get_model()\n",
        "        self.meta_model = self.get_meta()\n",
        "\n",
        "    def get_model(self) -> Union[GCN4MAML, SAGE4MAML]:\n",
        "        \"\"\"Return the model to use with the MetaModel\"\"\"\n",
        "        models = {'sage': SAGE4MAML, 'gcn': GCN4MAML}\n",
        "        model = models[self.model_name](num_classes=self.config.TRAIN_WAY).to(DEVICE)\n",
        "        print(f\"Creating model of type {model.__class__.__name__}\")\n",
        "        return model\n",
        "\n",
        "    def get_meta(self) -> AdaptiveStepMAML:\n",
        "        \"\"\"Return the meta model\"\"\"\n",
        "        print(f\"Creating the AS-MAML model\")\n",
        "        return AdaptiveStepMAML(self.model,\n",
        "                                inner_lr=self.config.INNER_LR,\n",
        "                                outer_lr=self.config.OUTER_LR,\n",
        "                                stop_lr=self.config.STOP_LR,\n",
        "                                weight_decay=self.config.WEIGHT_DECAY).to(DEVICE)\n",
        "\n",
        "    def get_dataloaders(self) -> Tuple[FewShotDataLoader, FewShotDataLoader]:\n",
        "        \"\"\"Return train and validation dataloaders\"\"\"\n",
        "        print(\"--- Creating the DataLoader for Training ---\")\n",
        "        train_dataloader = get_dataloader(\n",
        "            ds=self.train_ds, n_way=self.config.TRAIN_WAY, k_shot=self.config.TRAIN_SHOT,\n",
        "            n_query=self.config.TRAIN_QUERY, epoch_size=self.config.TRAIN_EPISODE,\n",
        "            shuffle=True, batch_size=1\n",
        "        )\n",
        "\n",
        "        print(\"--- Creating the DataLoader for Validation ---\")\n",
        "        validation_dataloader = get_dataloader(\n",
        "            ds=self.val_ds, n_way=self.config.TEST_WAY, k_shot=self.config.VAL_SHOT,\n",
        "            n_query=self.config.VAL_QUERY, epoch_size=self.config.VAL_EPISODE,\n",
        "            shuffle=True, batch_size=1\n",
        "        )\n",
        "\n",
        "        return train_dataloader, validation_dataloader\n",
        "    \n",
        "    def run_one_step_train(\n",
        "        self, support_data: gdata.Data, query_data: gdata.Data, train_accs: List[float],\n",
        "        train_total_losses: List[float], train_final_losses: List[float], loop_counter: int\n",
        "    ) -> None:\n",
        "        \"\"\"Run one episode, i.e. one or more tasks, of training\"\"\"\n",
        "        # Set support and query data to the GPU\n",
        "        if DEVICE != \"cpu\":\n",
        "            support_data = support_data.pin_memory()\n",
        "            query_data = query_data.pin_memory()\n",
        "            \n",
        "        support_data = support_data.to(DEVICE)\n",
        "        query_data = query_data.to(DEVICE)\n",
        "\n",
        "        accs, step, final_loss, total_loss, _, _, _, _ = self.meta_model(\n",
        "            support_data, query_data\n",
        "        )\n",
        "\n",
        "        train_accs.append(accs[step])\n",
        "        train_final_losses.append(final_loss)\n",
        "        train_total_losses.append(total_loss)\n",
        "\n",
        "        if (loop_counter + 1) % 50 == 0:\n",
        "            print(f\"({loop_counter + 1})\" + \" Mean Accuracy: {:.6f}, Mean Final Loss: {:.6f}, Mean Total Loss: {:.6f}\".format(\n",
        "                np.mean(train_accs), np.mean(train_final_losses), np.mean(train_total_losses)\n",
        "                ))\n",
        "            \n",
        "    def run_one_step_validation(self, support_data: gdata.Data, \n",
        "                                      query_data: gdata.Data, \n",
        "                                      val_accs: List[float], \n",
        "                                      loop_counter: int) -> None:\n",
        "        \"\"\"Run one episode, i.e. one or more tasks, of validation\"\"\"\n",
        "        if DEVICE != \"cpu\":\n",
        "            support_data = support_data.pin_memory()\n",
        "            query_data = query_data.pin_memory()\n",
        "\n",
        "        support_data = support_data.to(DEVICE)\n",
        "        query_data = query_data.to(DEVICE)\n",
        "        \n",
        "        accs, step, _, scores, query_losses = self.meta_model.finetuning(support_data, query_data)\n",
        "        acc = get_max_acc(accs, step, scores, self.config.MIN_STEP, self.config.MAX_STEP)\n",
        "\n",
        "        val_accs.append(accs[step])\n",
        "        if (loop_counter + 1) % 50 == 0:\n",
        "            printable_string = f\"Test Number {loop_counter + 1}\\n\" + \\\n",
        "                                \"\\tQuery Losses[{l}]: {query_losses}\\n\\tAccuracies {step}: {accs}\\n\\tMax Accuracy: {max_acc}\\n\".format(\n",
        "                                    l=len(query_losses), query_losses=query_losses, step=step,\n",
        "                                    accs=np.array([accs[i] for i in range(0, step + 1)]), max_acc=acc\n",
        "                                )\n",
        "\n",
        "            print(printable_string)\n",
        "    \n",
        "    @elapsed_time\n",
        "    def optimize(self):\n",
        "        \"\"\"Run the optimization (fitting)\"\"\"\n",
        "        train_dl, val_dl = self.get_dataloaders()\n",
        "        max_val_acc = 0\n",
        "        print(\"=\" * 40 + \" Starting Optimization \" + \"=\" * 40)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            setup_seed(epoch)\n",
        "            print(\"=\" * 103)\n",
        "            print(\"Epoch Number {:04d}\".format(epoch))\n",
        "\n",
        "            self.meta_model.train()\n",
        "            train_accs, train_final_losses, train_total_losses, val_accs = [], [], [], []\n",
        "\n",
        "            print(\"Training Phase\")\n",
        "\n",
        "            for i, data in enumerate(tqdm(train_dl)):\n",
        "                support_data, query_data = data\n",
        "                self.run_one_step_train(\n",
        "                    support_data=support_data, query_data=query_data,\n",
        "                    train_accs=train_accs, train_total_losses=train_total_losses,\n",
        "                    train_final_losses=train_final_losses, loop_counter=i\n",
        "                )\n",
        "            \n",
        "            print(\"Ended Training Phase\")\n",
        "            print(\"Validation Phase\")\n",
        "\n",
        "            self.meta_model.eval()\n",
        "            for i, data in enumerate(tqdm(val_dl)):\n",
        "                support_data, query_data = data\n",
        "                self.run_one_step_validation(\n",
        "                    support_data=support_data, query_data=query_data,\n",
        "                    val_accs=val_accs, loop_counter=i\n",
        "                )\n",
        "            \n",
        "            print(\"Ended Validation Phase\")\n",
        "\n",
        "            val_acc_avg = np.mean(val_accs)\n",
        "            train_acc_avg = np.mean(train_accs)\n",
        "            train_loss_avg = np.mean(train_final_losses)\n",
        "            val_acc_ci95 = 1.96 * np.std(np.array(val_accs)) / np.sqrt(self.config.VAL_EPISODE)\n",
        "\n",
        "            if val_acc_avg > max_val_acc:\n",
        "                max_val_acc = val_acc_avg\n",
        "                printable_string = \"Epoch(***Best***) {:04d}\\n\".format(epoch)\n",
        "\n",
        "                torch.save({\n",
        "                        'epoch': epoch, \n",
        "                        'embedding': self.meta_model.state_dict()\n",
        "                    }, os.path.join(MODELS_SAVE_PATH, f'{self.dataset_name}_BestModel.pth')\n",
        "                )\n",
        "            else :\n",
        "                printable_string = \"Epoch {:04d}\\n\".format(epoch)\n",
        "            \n",
        "            printable_string += \"\\tAvg Train Loss: {:.6f}, Avg Train Accuracy: {:.6f}\\n\".format(train_loss_avg, train_acc_avg) + \\\n",
        "                                \"\\tAvg Validation Accuracy: {:.2f} ±{:.26f}\\n\".format(val_acc_avg, val_acc_ci95) + \\\n",
        "                                \"\\tMeta Learning Rate: {:.6f}\\n\".format(self.meta_model.get_meta_learning_rate()) + \\\n",
        "                                \"\\tBest Current Validation Accuracy: {:.2f}\".format(max_val_acc)\n",
        "\n",
        "            print(printable_string)\n",
        "            self.meta_model.adapt_meta_learning_rate(train_loss_avg)\n",
        "\n",
        "        print(\"Optimization Finished\")\n",
        "\n",
        "\n",
        "class Tester:\n",
        "    \"\"\"Class for run tests using the best model from training\"\"\"\n",
        "    def __init__(self, test_ds: GraphDataset, best_model_path: str, config_class: object=ASMAMLConfig,\n",
        "                       dataset_name: str=\"TRIANGLES\", model_name: str=\"sage\") -> None:\n",
        "        self.test_ds = test_ds\n",
        "        self.dataset_name = dataset_name\n",
        "        self.model_name = model_name\n",
        "        self.best_model_path = best_model_path\n",
        "        self.config = config_class\n",
        "\n",
        "        self.model = self.get_model()\n",
        "        self.meta_model = self.get_meta()\n",
        "\n",
        "        # Using the pre-trained model, i.e. the best model resulted during training\n",
        "        saved_models = torch.load(self.best_model_path)\n",
        "        self.meta_model.load_state_dict(saved_models[\"embedding\"])\n",
        "        self.model = self.meta_model.net\n",
        "\n",
        "    \n",
        "    def get_model(self) -> Union[GCN4MAML, SAGE4MAML]:\n",
        "        \"\"\"Return the model to use with the MetaModel\"\"\"\n",
        "        models = {'sage': SAGE4MAML, 'gcn': GCN4MAML}\n",
        "        model = models[self.model_name](num_classes=self.config.TRAIN_WAY).to(DEVICE)\n",
        "        print(f\"Creating model of type {model.__class__.__name__}\")\n",
        "        return model\n",
        "\n",
        "    def get_meta(self) -> AdaptiveStepMAML:\n",
        "        \"\"\"Return the meta model\"\"\"\n",
        "        print(f\"Creating the AS-MAML model\")\n",
        "        return AdaptiveStepMAML(self.model,\n",
        "                                inner_lr=self.config.INNER_LR,\n",
        "                                outer_lr=self.config.OUTER_LR,\n",
        "                                stop_lr=self.config.STOP_LR,\n",
        "                                weight_decay=self.config.WEIGHT_DECAY,\n",
        "                                paper=self.paper).to(DEVICE)\n",
        "    \n",
        "    def run_one_step_test(self, support_data: gdata.Data, query_data: gdata.Data, \n",
        "                                val_accs: List[float], query_losses_list: List[float]) -> None:\n",
        "        \"\"\"Run one single step of testing\"\"\"\n",
        "        if DEVICE != \"cpu\":\n",
        "            support_data = support_data.pin_memory()\n",
        "            query_data = query_data.pin_memory()\n",
        "\n",
        "        support_data = support_data.to(DEVICE)\n",
        "        query_data = query_data.to(DEVICE)\n",
        "\n",
        "        accs, step, _, _, query_losses = self.meta_model.finetunning(support_data, query_data)\n",
        "\n",
        "        val_accs.append(accs[step])\n",
        "        query_losses_list.extend(query_losses)\n",
        "    \n",
        "    def get_dataloader(self) -> FewShotDataLoader:\n",
        "        \"\"\"Return test dataloader\"\"\"\n",
        "        print(\"--- Creating the DataLoader for Testing ---\")\n",
        "        test_dataloader = get_dataloader(\n",
        "            ds=self.test_ds, n_way=self.config.TEST_WAY, k_shot=self.config.VAL_SHOT,\n",
        "            n_query=self.config.VAL_QUERY, epoch_size=self.config.VAL_EPISODE,\n",
        "            shuffle=True, batch_size=1\n",
        "        )\n",
        "\n",
        "        return test_dataloader\n",
        "    \n",
        "    @elapsed_time\n",
        "    def test(self):\n",
        "        \"\"\"Run testing\"\"\"\n",
        "        setup_seed(1)\n",
        "\n",
        "        test_dl = self.get_dataloader()\n",
        "\n",
        "        print(\"=\" * 40 + \" Starting Testing \" + \"=\" * 40)\n",
        "\n",
        "        val_accs = []\n",
        "        query_losses_list = []\n",
        "        self.meta_model.eval()\n",
        "\n",
        "        for _, data in enumerate(tqdm(test_dl)):\n",
        "            support_data, query_data = data\n",
        "            self.run_one_step_test(support_data, query_data, val_accs, query_losses_list)\n",
        "        \n",
        "        val_acc_avg = np.mean(val_accs)\n",
        "        val_acc_ci95 = 1.96 * np.std(np.array(val_accs)) / np.sqrt(self.config.VAL_EPISODE)\n",
        "        query_losses_avg = np.array(query_losses_list).mean()\n",
        "        query_losses_min = np.array(query_losses_list).min()\n",
        "\n",
        "        printable_string = (\n",
        "            \"\\nTEST FINISHED --- Results\\n\"        +\n",
        "            \"\\tTesting Accuracy: {:.2f} ±{:.2f}\\n\" + \n",
        "            \"\\tQuery Losses Avg: {:.6f}\\n\"         +\n",
        "            \"\\tMin Query Loss: {:.6f}\\n\"\n",
        "            ).format(\n",
        "                val_acc_avg, val_acc_ci95,\n",
        "                query_losses_avg, query_losses_min\n",
        "            )\n",
        "\n",
        "        print(printable_string)"
      ],
      "metadata": {
        "id": "A7y4o836DKQv"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training and then testing\n",
        "torch.set_printoptions(edgeitems=EDGELIMIT_PRINT)\n",
        "\n",
        "dataset_name = DEFAULT_DATASET\n",
        "train_ds, test_ds, val_ds, _ = get_dataset(\n",
        "    download=DOWNLOAD_DATASET, \n",
        "    data_dir=DATA_PATH,\n",
        "    dataset_name=dataset_name\n",
        ")\n",
        "\n",
        "print(\"--- Datasets ---\")\n",
        "print(\"\\n- Train: \", train_ds)\n",
        "print(\"- Test : \", test_ds)\n",
        "print(\"- Validation: \", val_ds)\n",
        "print()\n",
        "\n",
        "print(\"--- Configurations ---\")\n",
        "\n",
        "configurations = (\"\\nDEVICE: {device}\\n\"                            +\n",
        "                    \"DATASET NAME: {dataset_name}\\n\"                + \n",
        "                    \"TRAIN SUPPORT SIZE: {train_support_size}\\n\"    +\n",
        "                    \"TRAIN QUERY SIZE: {train_query_size}\\n\"        +\n",
        "                    \"VALIDATION SUPPORT SIZE: {val_support_size}\\n\" +\n",
        "                    \"VALIDATION QUERY SIZE: {val_query_size}\\n\"     +\n",
        "                    \"TEST SUPPORT SIZE: {test_support_size}\\n\"      +\n",
        "                    \"TEST QUERY SIZE: {test_query_size}\\n\"          +\n",
        "                    \"TRAIN EPISODE: {train_episode}\\n\"              +\n",
        "                    \"VALIDATION EPISODE: {val_episode}\\n\"           +\n",
        "                    \"NUMBER OF EPOCHS: {number_of_epochs}\\n\"        +\n",
        "                    \"BATCH PER EPISODES: {batch_per_episodes}\\n\"\n",
        "    ).format(\n",
        "        device=DEVICE, dataset_name=dataset_name,\n",
        "        train_support_size=f\"{ASMAMLConfig.TRAIN_WAY} x {ASMAMLConfig.TRAIN_SHOT}\",\n",
        "        train_query_size=f\"{ASMAMLConfig.TRAIN_WAY} x {ASMAMLConfig.TRAIN_QUERY}\",\n",
        "        val_support_size=f\"{ASMAMLConfig.TEST_WAY} x {ASMAMLConfig.VAL_SHOT}\",\n",
        "        val_query_size=f\"{ASMAMLConfig.TEST_WAY} x {ASMAMLConfig.VAL_QUERY}\",\n",
        "        test_support_size=f\"{ASMAMLConfig.TEST_WAY} x {ASMAMLConfig.VAL_SHOT}\",\n",
        "        test_query_size=f\"{ASMAMLConfig.TEST_WAY} x {ASMAMLConfig.VAL_QUERY}\",\n",
        "        train_episode=ASMAMLConfig.TRAIN_EPISODE, val_episode=ASMAMLConfig.VAL_EPISODE,\n",
        "        number_of_epochs=ASMAMLConfig.EPOCHS, batch_per_episodes=ASMAMLConfig.BATCH_PER_EPISODES\n",
        "    )\n",
        "\n",
        "print(configurations)\n",
        "\n",
        "optimizer = Optimizer(train_ds, val_ds, epochs=ASMAMLConfig.EPOCHS, dataset_name=dataset_name)\n",
        "optimizer.optimize()\n",
        "\n",
        "best_model_path = os.path.join(MODELS_SAVE_PATH, f\"{dataset_name}_BestModel.pth\")\n",
        "tester = Tester(test_ds, best_model_path)\n",
        "tester.test()"
      ],
      "metadata": {
        "id": "madCXaVyXNup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "am9uzaLWYw0R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}