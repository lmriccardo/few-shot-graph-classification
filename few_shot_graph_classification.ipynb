{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "few-shot-graph-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Few-Shot Graph Classification\n",
        "\n",
        "Most of the graph classification task overlook the scarcity of labeled graph in many situations. To overcome this problem, *Few-Shot Learning* is started being used. It is a type of Machine Learning method where the training dataset contains limited information. The general practice is to feed the machine learning model with as much data as possible, since this leads to better predictions. However, few-shot learning aims to build accurate machine learning models with less training data. Few-Shot Learning, and in particular in this case Few-shot classification, aims to reduce the cost of gain and label a huge amount of data.\n",
        "\n",
        "*Which is the idea behind Few-Shot Learning*? (on graphs) Given graph data $\\mathcal{G} = \\{(G_1, \\mathbf{y}_1), ..., (G_n, \\mathbf{y}_n)\\}$, we split it into train, $\\{(G^{train}, \\mathbf{y}^{train})\\}$, and test dataset, $\\{(G^{test}, \\mathbf{y}^{test})\\}$. Notice that $\\mathbf{y}^{train}$ and $\\mathbf{y}^{test}$ must have no common classes. For training we use episodic training method, this means that at training stage the algorithm sample a so-called *Task*, i.e., a pair (*support* set, *query* set) where the support set is $D_{sup}^{train} = \\{(G_i^{train}, \\mathbf{y}_{i}^{train})\\}_{i=1}^s$, where $s = N \\times K$, while the query set is $D_{que}^{train} = \\{(G_i^{train}, \\mathbf{y}_{i}^{train})\\}_{i=1}^q$, where $q$ is the number of query data. Given labeled support data, the goal is to predict the labels of query data. Note that in a single task, support data and query data share the same class space. This is also called **N-way-K-shot** learning, where **N** is the number of sampled classes and **K** is the number of samples for each of the N classes. At test stage when performing classification tasks on unseen classes, we firstly fine tune the meta-learner on the support data of test classes, then report classification performance on the test query set.\n",
        "\n",
        "In the following, I'm going to present some approaches in few-shot Learning. First, a *Meta-Learning Framework* based on Fast Weight Adaptation, taken from the paper [Adaptive-Step Graph Meta-Learner for Few-Shot Graph Classification](https://arxiv.org/pdf/2003.08246.pdf) (Ning Ma et al.). Second, I'm going to compare it with different GDA (graph data augmentation) techniques used to enrich the dataset for the novel classes (i.e., those with the less amount of data) taken from a second paper named [Graph Data Augmentation for Graph Machine Learning: A Survey](https://arxiv.org/pdf/2202.08871.pdf) (Tong Zhao et al.)."
      ],
      "metadata": {
        "id": "6YcW8xJl1woa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modules and Constants"
      ],
      "metadata": {
        "id": "ezvmZRr_RhpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install pyyaml==5.4.1\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "WGkCr5T9HyrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import (\n",
        "    Any, Dict, List, Tuple, \n",
        "    Union, Generic, Optional,\n",
        "    TypeVar\n",
        ")\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from functools import wraps\n",
        "import plotly.graph_objects as go\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "\n",
        "import torch_geometric.data as gdata\n",
        "import torch_geometric.loader as gloader\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
        "from torch_geometric.nn.inits import uniform\n",
        "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
        "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
        "from torch_goemetric.utils import (\n",
        "    add_remaining_self_loops, \n",
        "    add_self_loops, \n",
        "    remove_self_loops,\n",
        "    softmax\n",
        ")\n",
        "\n",
        "from torch_scatter import scatter_add\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "WdiE2uITSaS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRIANGLES_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/TRIANGLES.zip\"\n",
        "COIL_DEL_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/COIL-DEL.zip\"\n",
        "R52_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/R52.zip\"\n",
        "LETTER_HIGH_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/Letter-High.zip\"\n",
        "\n",
        "DATASETS = {\n",
        "    \"TRIANGLES\"   : TRIANGLES_ZIP_URL, \n",
        "    \"COIL-DEL\"    : COIL_DEL_ZIP_URL, \n",
        "    \"R52\"         : R52_ZIP_URL, \n",
        "    \"Letter-High\" : LETTER_HIGH_ZIP_URL\n",
        "}\n",
        "\n",
        "T = TypeVar('T')\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DOWNLOAD_DATASET = True\n",
        "SAVE_PICLKE  = True\n",
        "EDGELIMIT_PRINT = 2000\n",
        "\n",
        "NUM_FEATURES = {\"TRIANGLES\": 1, \"R52\": 1, \"Letter-High\": 2, \"COIL-DEL\": 2}\n",
        "\n",
        "\n",
        "class ASMAMLConfig:\n",
        "    NHID = 128\n",
        "    POOLING_RATIO = 0.5\n",
        "    DROPOUT_RATIO = 0.3\n",
        "\n",
        "    OUTER_LR     = 0.001\n",
        "    INNER_LR     = 0.01\n",
        "    STOP_LR      = 0.0001\n",
        "    WEIGHT_DECAY = 1E-05\n",
        "\n",
        "    MAX_STEP      = 15\n",
        "    MIN_STEP      = 5\n",
        "    STEP_TEST     = 15\n",
        "    FLEXIBLE_STEP = True\n",
        "    STEP_PENALITY = 0.001\n",
        "    USE_SCORE     = True\n",
        "    USE_GRAD      = False\n",
        "    USE_LOSS      = True\n",
        "\n",
        "    TRAIN_SHOT         = 10   # K-shot for training set\n",
        "    VAL_SHOT           = 10   # K-shot for validation (or test) set\n",
        "    TRAIN_QUERY        = 15   # Number of query for the training set\n",
        "    VAL_QUERY          = 15   # Number of query for the validation (or test) set\n",
        "    TRAIN_WAY          = 3    # N-way for training set\n",
        "    TEST_WAY           = 3    # N-way for test set\n",
        "    VAL_EPISODE        = 200  # Number of episodes for validation\n",
        "    TRAIN_EPISODE      = 200  # Number of episodes for training\n",
        "    BATCH_PER_EPISODES = 5    # How many batch per episode\n",
        "    EPOCHS             = 500  # How many epochs\n",
        "    PATIENCE           = 35\n",
        "    GRAD_CLIP          = 5\n",
        "\n",
        "    # Stop Control configurations\n",
        "    STOP_CONTROL_INPUT_SIZE = 2\n",
        "    STOP_CONTROL_HIDDEN_SIZE = 20"
      ],
      "metadata": {
        "id": "7vboF7-3VcvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "qs070YovWwAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scandir(root_path: str) -> List[str]:\n",
        "    \"\"\"Recursively scan a directory looking for files\"\"\"\n",
        "    root_path = os.path.abspath(root_path)\n",
        "    content = []\n",
        "    for file in os.listdir(root_path):\n",
        "        new_path = os.path.join(root_path, file)\n",
        "        if os.path.isfile(new_path):\n",
        "            content.append(new_path)\n",
        "            continue\n",
        "        \n",
        "        content += scandir(new_path)\n",
        "    \n",
        "    return content\n",
        "\n",
        "\n",
        "def download_zipped_data(url: str, path2extract: str, dataset_name: str) -> List[str]:\n",
        "    \"\"\"Download and extract a ZIP file from URL. Return the content filename\"\"\"\n",
        "    logging.debug(f\"--- Downloading from {url} ---\")\n",
        "    response = requests.get(url)\n",
        "\n",
        "    abs_path2extract = os.path.abspath(path2extract)\n",
        "    zip_path = os.path.join(abs_path2extract, f\"{dataset_name}.zip\")\n",
        "    with open(zip_path, mode=\"wb\") as iofile:\n",
        "        iofile.write(response.content)\n",
        "\n",
        "    # Extract the file\n",
        "    logging.debug(\"--- Extracting files from the archive ---\")\n",
        "    with zipfile.ZipFile(zip_path, mode=\"r\") as zip_ref:\n",
        "        zip_ref.extractall(abs_path2extract)\n",
        "\n",
        "    logging.debug(f\"--- Removing {zip_path} ---\")\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    return scandir(os.path.join(path2extract, dataset_name))\n",
        "\n",
        "\n",
        "def delete_data_folder(path2delete: str) -> None:\n",
        "    \"\"\"Delete the folder containing data\"\"\"\n",
        "    logging.debug(\"--- Removing Content Data ---\")\n",
        "    shutil.rmtree(path2delete)\n",
        "    logging.debug(\"--- Removed Finished Succesfully ---\")"
      ],
      "metadata": {
        "id": "hiPgKuQqXDNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def elapsed_time(func):\n",
        "    \"\"\"Just a simple wrapper for counting elapsed time from start to end\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        logging.debug(\"Elapsed Time: {:.6f}\".format(end - start))\n",
        "    \n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "7DFVuisvXFHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_with_pickle(path2save: str, content: Any) -> None:\n",
        "    \"\"\"Save content inside a .pickle file denoted by path2save\"\"\"\n",
        "    path2save = path2save + \".pickle\" if \".pickle\" not in path2save else path2save\n",
        "    with open(path2save, mode=\"wb\") as iostream:\n",
        "        pickle.dump(content, iostream)\n",
        "\n",
        "\n",
        "def load_with_pickle(path2load: str) -> Any:\n",
        "    \"\"\"Load a content from a .pickle file\"\"\"\n",
        "    with open(path2load, mode=\"rb\") as iostream:\n",
        "        return pickle.load(iostream)"
      ],
      "metadata": {
        "id": "fN6UJb7jXJ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "JmagKA3bXL1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(G : Union[nx.Graph, nx.DiGraph], name: str) -> None:\n",
        "    \"\"\"\n",
        "    Plot a graph\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    graph : Union[nx.Graph, nx.DiGraph]\n",
        "        Just a nx.Graph object\n",
        "    name  : str\n",
        "        The name of the graph\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Getting the 3D Spring layout\n",
        "    layout = nx.spring_layout(G, dim=3, seed=18)\n",
        "    \n",
        "    # Getting nodes coordinate\n",
        "    x_nodes = [layout[i][0] for i in layout]  # x-coordinates of nodes\n",
        "    y_nodes = [layout[i][1] for i in layout]  # y-coordinates of nodes\n",
        "    z_nodes = [layout[i][2] for i in layout]  # z-coordinates of nodes\n",
        "    \n",
        "    # Getting a list of edges and create a list with coordinates\n",
        "    elist = G.edges()\n",
        "    x_edges, y_edges, z_edges = [], [], []\n",
        "    for edge in elist:\n",
        "        x_edges += [layout[edge[0]][0], layout[edge[1]][0], None]\n",
        "        y_edges += [layout[edge[0]][1], layout[edge[1]][1], None]\n",
        "        z_edges += [layout[edge[0]][2], layout[edge[1]][2], None]\n",
        "\n",
        "    colors = np.linspace(0, len(x_nodes))\n",
        "        \n",
        "    # Create a trace for the edges\n",
        "    etrace = go.Scatter3d(x=x_edges,\n",
        "                          y=y_edges,\n",
        "                          z=z_edges,\n",
        "                          mode='lines',\n",
        "                          line=dict(color='rgb(125,125,125)', width=1),\n",
        "                          hoverinfo='none'\n",
        "                         )\n",
        "    \n",
        "    # Create a trace for the nodes\n",
        "    ntrace = go.Scatter3d(x=x_nodes,\n",
        "                          y=y_nodes,\n",
        "                          z=z_nodes,\n",
        "                          mode='markers',\n",
        "                          marker=dict(\n",
        "                              symbol='circle',\n",
        "                              size=6,\n",
        "                              color=colors,\n",
        "                              colorscale='Viridis',\n",
        "                              line=dict(color='rgb(50,50,50)', width=.5)),\n",
        "                          text=list(layout.keys()),\n",
        "                          hoverinfo='text'\n",
        "                         )\n",
        "    \n",
        "    # Set the axis\n",
        "    axis = dict(showbackground=False,\n",
        "                showline=False,\n",
        "                zeroline=False,\n",
        "                showgrid=False,\n",
        "                showticklabels=False,\n",
        "                title='')\n",
        "    \n",
        "    # Create a layout for the plot\n",
        "    go_layout = go.Layout(title=f\"{name} Network Graph\",\n",
        "                          width=600,\n",
        "                          height=600,\n",
        "                          showlegend=False,\n",
        "                          scene=dict(xaxis=dict(axis),\n",
        "                                     yaxis=dict(axis),\n",
        "                                     zaxis=dict(axis)),\n",
        "                          margin=dict(t=100),\n",
        "                          hovermode='closest'\n",
        "                         )\n",
        "    \n",
        "    # Plot\n",
        "    data = [etrace, ntrace]\n",
        "    fig = go.Figure(data=data, layout=go_layout)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "hA0a8aVGX6kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_edge_indexes(data_list: List[gdata.Data]) -> List[gdata.Data]:\n",
        "    \"\"\"\n",
        "    Takes as input a bunch of :obj:`torch_geometric.data.Data` and renames\n",
        "    each edge node (x, y) from 1 to total number of nodes. For instance, if we have\n",
        "    this edge_index = [[1234, 1235, 1236, 1237], [1238, 1239, 1230,1241]] this became\n",
        "    egde_index = [[0, 1, 2, 3],[4, 5, 6, 7]] and so on. \n",
        "\n",
        "    :param data_list: the list of :obj:`torch_geometric.data.Data`\n",
        "    :return: a new list of data\n",
        "    \"\"\"\n",
        "    # First of all let's compute the total number of nodes overall\n",
        "    total_number_nodes = 0\n",
        "    for data in data_list:\n",
        "        total_number_nodes += data.x.shape[0]\n",
        "    \n",
        "    # Generate the new nodes\n",
        "    nodes = torch.arange(0, total_number_nodes)\n",
        "    \n",
        "    # Takes the old nodes from the edge_index attribute\n",
        "    old_nodes = None\n",
        "    for data in data_list:\n",
        "        x, y = data.edge_index\n",
        "        x = torch.hstack((x, y)).unique(sorted=False)\n",
        "        \n",
        "        if old_nodes is None:\n",
        "            old_nodes = x\n",
        "            continue\n",
        "    \n",
        "        old_nodes = torch.hstack((old_nodes, x))\n",
        "    \n",
        "    # Create mapping from old to new nodes\n",
        "    mapping = dict(zip(old_nodes.tolist(), nodes.tolist()[:old_nodes.shape[0]]))\n",
        "    \n",
        "    # Finally, map the new nodes\n",
        "    for data in data_list:\n",
        "        x, y = data.edge_index\n",
        "        new_x = torch.tensor(list(map(lambda x: mapping[x], x.tolist())), dtype=x.dtype, device=x.device)\n",
        "        new_y = torch.tensor(list(map(lambda y: mapping[y], y.tolist())), dtype=y.dtype, device=y.device)\n",
        "        new_edge_index = torch.vstack((new_x, new_y))\n",
        "        data.edge_index = new_edge_index\n",
        "    \n",
        "    return data_list\n",
        "\n",
        "\n",
        "def data_batch_collate(data_list: List[gdata.Data]) -> gdata.Data:\n",
        "    \"\"\"\n",
        "    Takes as input a list of data and create a new :obj:`torch_geometric.data.Data`\n",
        "    collating all together. This is a replacement for torch_geometric.data.Batch.from_data_list\n",
        "\n",
        "    :param data_list: a list of torch_geometric.data.Data objects\n",
        "    :return: a new torch_geometric.data.Data object\n",
        "    \"\"\"\n",
        "    x = None\n",
        "    edge_index = None\n",
        "    batch = []\n",
        "    num_graphs = 0\n",
        "    y = None\n",
        "\n",
        "    # Do a shuffle of the data\n",
        "    random.shuffle(data_list)\n",
        "    \n",
        "    for i_data, data in enumerate(data_list):\n",
        "        x = data.x if x is None else torch.vstack((x, data.x))\n",
        "        edge_index = data.edge_index if edge_index is None else torch.hstack((edge_index, data.edge_index))\n",
        "        batch += [i_data] * data.x.shape[0]\n",
        "        num_graphs += 1\n",
        "        y = data.y if y is None else torch.hstack((y, data.y))\n",
        "\n",
        "    # Create a mapping between y and a range(0, num_classes_of_y)\n",
        "    # First we need to compute how many classes do we have\n",
        "    num_classes = y.unique().shape[0]\n",
        "    classes = list(range(0, num_classes))\n",
        "    mapping = dict(zip(y.unique(sorted=False).tolist(), classes))\n",
        "    \n",
        "    # This mapping is necessary when computing the cross-entropy-loss\n",
        "    new_y = torch.tensor(list(map(lambda x: mapping[x], y.tolist())), dtype=y.dtype, device=y.device)\n",
        "    \n",
        "    data_batch = gdata.Data(\n",
        "        x=x, edge_index=edge_index, batch=torch.tensor(batch),\n",
        "        y=new_y, num_graphs=num_graphs, old_classes_mapping=mapping\n",
        "    )\n",
        "\n",
        "    return data_batch\n",
        "\n",
        "\n",
        "def task_sampler_uncollate(task_sampler: 'TaskBatchSampler', data_batch: gdata.Batch):\n",
        "    \"\"\"\n",
        "    Takes as input the task sampler and a batch containing both the \n",
        "    support and the query set. It returns two different DataBatch\n",
        "    respectively for support and query_set.\n",
        "\n",
        "    Assume L = [x1, x2, x3, ..., xN] is the data_batch\n",
        "    each xi is a graph. Moreover, we have that\n",
        "    L[0:K] = support sample for the first class\n",
        "    L[K+1:K+Q] = query sample for the first class\n",
        "    In general, we have that \n",
        "\n",
        "            L[i * (K + Q) : (i + 1) * (K + Q)]\n",
        "\n",
        "    is the (support, query) pair for the i-th class\n",
        "    Finally, the first batch is the one that goes from\n",
        "    L[0 : N * (K + Q)], so\n",
        "\n",
        "            L[i * N * (K + Q) : (i + 1) * N * (K + Q)]\n",
        "\n",
        "    is the i-th batch.\n",
        "\n",
        "    :param task_sampler: The task sampler\n",
        "    :param data_batch: a batch with support and query set\n",
        "    :return: support batch, query batch\n",
        "    \"\"\"\n",
        "    n_way = task_sampler.task_sampler.n_way\n",
        "    k_shot = task_sampler.task_sampler.k_shot\n",
        "    n_query = task_sampler.task_sampler.n_query\n",
        "    task_batch_size = task_sampler.task_batch_size\n",
        "\n",
        "    total_support_query_number = n_way * (k_shot + n_query)\n",
        "    support_plus_query = k_shot + n_query\n",
        "\n",
        "    # Initialize batch list for support and query set\n",
        "    support_data_batch = []\n",
        "    query_data_batch = []\n",
        "\n",
        "    # I know how many batch do I have, so\n",
        "    for batch_number in range(task_batch_size):\n",
        "\n",
        "        # I also know how many class do I have in a task\n",
        "        for class_number in range(n_way):\n",
        "\n",
        "            # First of all let's take the i-th batch\n",
        "            data_batch_slice = slice(\n",
        "                batch_number * total_support_query_number,\n",
        "                (batch_number + 1) * total_support_query_number\n",
        "            )\n",
        "            data_batch_per_batch = data_batch[data_batch_slice]\n",
        "\n",
        "            # Then let's take the (support, query) pair for a class\n",
        "            support_query_slice = slice(\n",
        "                class_number * support_plus_query,\n",
        "                (class_number + 1) * support_plus_query\n",
        "            )\n",
        "            support_query_data = data_batch_per_batch[support_query_slice]\n",
        "\n",
        "            # Divide support from query\n",
        "            support_data = support_query_data[:k_shot]\n",
        "            query_data = support_query_data[k_shot:support_plus_query]\n",
        "\n",
        "            support_data_batch += support_data\n",
        "            query_data_batch += query_data\n",
        "    \n",
        "    # Rename the edges\n",
        "    support_data = data_batch_collate(rename_edge_indexes(support_data_batch))\n",
        "    query_data   = data_batch_collate(rename_edge_indexes(query_data_batch))\n",
        "\n",
        "    # Create new DataBatchs and return\n",
        "    return support_data, query_data"
      ],
      "metadata": {
        "id": "QcbwhI_-X7_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_acc(accs, step, scores, min_step, test_step):\n",
        "    step = np.argmax(scores[min_step - 1 : test_step]) + min_step - 1\n",
        "    return accs[step]\n",
        "\n",
        "\n",
        "def get_batch_number(databatch, i_batch, n_way, k_shot):\n",
        "    \"\"\"From a N batch takes the i-th batch\"\"\"\n",
        "    dim_databatch = n_way * k_shot\n",
        "    indices = torch.arange(0, config.BATCH_PER_EPISODES)\n",
        "    return gdata.Batch.from_data_list(databatch[indices * dim_databatch + i_batch])\n",
        "\n",
        "\n",
        "def glorot(tensor):\n",
        "    \"\"\"Apply the Glorot NN initialization (also called Xavier)\"\"\"\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    \"\"\"Fill a tensor with zeros if it is not Null\"\"\"\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)"
      ],
      "metadata": {
        "id": "lQ10ebDyYOdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset, Sampler and DataLoader"
      ],
      "metadata": {
        "id": "Y1K8uYH0YarQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphDataset(gdata.Dataset):\n",
        "    def __init__(self, graphs_ds: Dict[str, Tuple[nx.Graph, str]]) -> None:\n",
        "        super(GraphDataset, self).__init__()\n",
        "        self.graphs_ds = graphs_ds\n",
        "\n",
        "    @classmethod\n",
        "    def get_dataset(cls, attributes: List[Any], data: Dict[str, Any]) -> 'GraphDataset':\n",
        "        \"\"\"\n",
        "        Returns a new instance of GraphDataset filled with graphs inside data. 'attributes'\n",
        "        is the list with all the attributes (not only those beloging to nodes in 'data').\n",
        "\n",
        "        :param data: a dictionary with label2graphs, graph2nodes and graph2edges\n",
        "        :param attributes: a list with node attributes\n",
        "        :return: a new instance of GraphDataset\n",
        "        \"\"\"\n",
        "        graphs = dict()\n",
        "\n",
        "        label2graphs = data[\"label2graphs\"]\n",
        "        graph2nodes  = data[\"graph2nodes\"]\n",
        "        graph2edges  = data[\"graph2edges\"]\n",
        "\n",
        "        for label, graph_list in label2graphs.items():\n",
        "            for graph_id in graph_list:\n",
        "                graph_nodes = graph2nodes[graph_id]\n",
        "                graph_edges = graph2edges[graph_id]\n",
        "                nodes_attributes = [[attributes[node_id - 1]] for node_id in graph_nodes]\n",
        "                nodes = []\n",
        "                for node, attribute in zip(graph_nodes, nodes_attributes):\n",
        "                    nodes.append((node, {f\"attr{i}\" : a for i, a in enumerate(attribute)}))\n",
        "\n",
        "                g = nx.Graph()\n",
        "                g.add_edges_from(graph_edges)\n",
        "                g.add_nodes_from(nodes)\n",
        "            \n",
        "                graphs[graph_id] = (g, label)\n",
        "\n",
        "        graph_dataset = super(GraphDataset, cls).__new__(cls)\n",
        "        graph_dataset.__init__(graphs)\n",
        "\n",
        "        return graph_dataset\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"GraphDataset(classes={set(self.targets().tolist())},n_graphs={self.len()})\"\n",
        "\n",
        "    def indices(self) -> List[str]:\n",
        "        \"\"\" Return all the graph IDs \"\"\"\n",
        "        return list(self.graphs_ds.keys())\n",
        "\n",
        "    def len(self) -> int:\n",
        "        return len(self.graphs_ds.keys())\n",
        "\n",
        "    def targets(self) -> torch.Tensor:\n",
        "        \"\"\" Return all the labels \"\"\"\n",
        "        targets = []\n",
        "        for _, graph in self.graphs_ds.items():\n",
        "            targets.append(int(graph[1]))\n",
        "\n",
        "        return torch.tensor(targets)\n",
        "\n",
        "    def get(self, idx: Union[int, str]) -> gdata.Data:\n",
        "        \"\"\" Return (Graph object, Adjacency matrix and label) of a graph \"\"\"\n",
        "        if isinstance(idx, str):\n",
        "            idx = int(idx)\n",
        "\n",
        "        graph = self.graphs_ds[idx]\n",
        "        g, label = graph[0].to_directed(), graph[1]\n",
        "\n",
        "        # Retrieve nodes attributes\n",
        "        attrs = list(g.nodes(data=True))\n",
        "        x = torch.tensor([list(map(int, a.values())) for _, a in attrs], dtype=torch.float)\n",
        "\n",
        "        # Retrieve edges\n",
        "        edge_index = torch.tensor([list(e) for e in g.edges], dtype=torch.long) \\\n",
        "                          .t()                                                  \\\n",
        "                          .contiguous()\n",
        "\n",
        "        # Retrieve ground trouth labels\n",
        "        y = torch.tensor([int(label)], dtype=torch.int)\n",
        "\n",
        "        return gdata.Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "\n",
        "def generate_train_val_test(dataset_name: str,\n",
        "                            data_dir: Optional[str]=None, \n",
        "                            download: bool=True,\n",
        "                            download_folder: str=\"../data\"\n",
        ") -> Tuple[GraphDataset, GraphDataset, GraphDataset]:\n",
        "    \"\"\" Return dataset for training, validation and testing \"\"\"\n",
        "    logging.debug(\"--- Generating Train, Test and Validation datasets --- \")\n",
        "    \n",
        "    assert download or data_dir is not None, \"At least one between: data_dir and download must be given\"\n",
        "\n",
        "    node_attribute = None\n",
        "    test_file = None\n",
        "    train_file = None\n",
        "    val_file = None\n",
        "\n",
        "    if data_dir is not None:\n",
        "        node_attribute = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_node_attributes.pickle\")\n",
        "        test_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_test_set.pickle\")\n",
        "        train_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_train_set.pickle\")\n",
        "        val_file = os.path.join(data_dir, f\"{dataset_name}/{dataset_name}_val_set.pickle\")\n",
        "\n",
        "    if download:\n",
        "        node_attribute, test_file, train_file, val_file = download_zipped_data(\n",
        "            config.DATASETS[dataset_name], \n",
        "            download_folder, \n",
        "            dataset_name\n",
        "        )\n",
        "\n",
        "        data_dir = \"\\\\\".join(node_attribute.replace(\"\\\\\", \"/\").split(\"/\")[:-2])\n",
        "\n",
        "    node_attribute_data = load_with_pickle(node_attribute)\n",
        "    test_data = load_with_pickle(test_file)\n",
        "    train_data = load_with_pickle(train_file)\n",
        "    val_data = load_with_pickle(val_file)\n",
        "\n",
        "    train_ds = GraphDataset.get_dataset(node_attribute_data, train_data)\n",
        "    test_ds  = GraphDataset.get_dataset(node_attribute_data,  test_data)\n",
        "    val_ds   = GraphDataset.get_dataset(node_attribute_data,   val_data)\n",
        "\n",
        "    return train_ds, test_ds, val_ds, data_dir"
      ],
      "metadata": {
        "id": "FI3PQiGDYaVn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}