{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "few-shot-graph-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Few-Shot Graph Classification\n",
        "\n",
        "Most of the graph classification task overlook the scarcity of labeled graph in many situations. To overcome this problem, *Few-Shot Learning* is started being used. It is a type of Machine Learning method where the training dataset contains limited information. The general practice is to feed the machine learning model with as much data as possible, since this leads to better predictions. However, few-shot learning aims to build accurate machine learning models with less training data. Few-Shot Learning, and in particular in this case Few-shot classification, aims to reduce the cost of gain and label a huge amount of data.\n",
        "\n",
        "*Which is the idea behind Few-Shot Learning*? (on graphs) Given graph data $\\mathcal{G} = \\{(G_1, \\mathbf{y}_1), ..., (G_n, \\mathbf{y}_n)\\}$, we split it into train, $\\{(G^{train}, \\mathbf{y}^{train})\\}$, and test dataset, $\\{(G^{test}, \\mathbf{y}^{test})\\}$. Notice that $\\mathbf{y}^{train}$ and $\\mathbf{y}^{test}$ must have no common classes. For training we use episodic training method, this means that at training stage the algorithm sample a so-called *Task*, i.e., a pair (*support* set, *query* set) where the support set is $D_{sup}^{train} = \\{(G_i^{train}, \\mathbf{y}_{i}^{train})\\}_{i=1}^s$, where $s = N \\times K$, while the query set is $D_{que}^{train} = \\{(G_i^{train}, \\mathbf{y}_{i}^{train})\\}_{i=1}^q$, where $q$ is the number of query data. Given labeled support data, the goal is to predict the labels of query data. Note that in a single task, support data and query data share the same class space. This is also called **N-way-K-shot** learning, where **N** is the number of sampled classes and **K** is the number of samples for each of the N classes. At test stage when performing classification tasks on unseen classes, we firstly fine tune the meta-learner on the support data of test classes, then report classification performance on the test query set.\n",
        "\n",
        "In the following, I'm going to present some approaches in few-shot Learning. First, a *Meta-Learning Framework* based on Fast Weight Adaptation, taken from the paper [Adaptive-Step Graph Meta-Learner for Few-Shot Graph Classification](https://arxiv.org/pdf/2003.08246.pdf) (Ning Ma et al.). Second, I'm going to compare it with different GDA (graph data augmentation) techniques used to enrich the dataset for the novel classes (i.e., those with the less amount of data) taken from a second paper named [Graph Data Augmentation for Graph Machine Learning: A Survey](https://arxiv.org/pdf/2202.08871.pdf) (Tong Zhao et al.)."
      ],
      "metadata": {
        "id": "6YcW8xJl1woa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modules and Constants"
      ],
      "metadata": {
        "id": "ezvmZRr_RhpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install pyyaml==5.4.1\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "WGkCr5T9HyrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import (\n",
        "    Any, Dict, List, Tuple, \n",
        "    Union, Generic, Optional,\n",
        "    TypeVar\n",
        ")\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from functools import wraps\n",
        "import plotly.graph_objects as go\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "\n",
        "import torch_geometric.data as gdata\n",
        "import torch_geometric.loader as gloader\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
        "from torch_geometric.nn.inits import uniform\n",
        "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
        "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
        "from torch_goemetric.utils import (\n",
        "    add_remaining_self_loops, \n",
        "    add_self_loops, \n",
        "    remove_self_loops,\n",
        "    softmax\n",
        ")\n",
        "\n",
        "from torch_scatter import scatter_add\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "WdiE2uITSaS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRIANGLES_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/TRIANGLES.zip\"\n",
        "COIL_DEL_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/COIL-DEL.zip\"\n",
        "R52_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/R52.zip\"\n",
        "LETTER_HIGH_ZIP_URL = \"https://cloud-storage.eu-central-1.linodeobjects.com/Letter-High.zip\"\n",
        "\n",
        "DATASETS = {\n",
        "    \"TRIANGLES\"   : TRIANGLES_ZIP_URL, \n",
        "    \"COIL-DEL\"    : COIL_DEL_ZIP_URL, \n",
        "    \"R52\"         : R52_ZIP_URL, \n",
        "    \"Letter-High\" : LETTER_HIGH_ZIP_URL\n",
        "}\n",
        "\n",
        "T = TypeVar('T')\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DOWNLOAD_DATASET = True\n",
        "SAVE_PICLKE  = True\n",
        "EDGELIMIT_PRINT = 2000\n",
        "\n",
        "NUM_FEATURES = {\"TRIANGLES\": 1, \"R52\": 1, \"Letter-High\": 2, \"COIL-DEL\": 2}\n",
        "\n",
        "\n",
        "class ASMAMLConfig:\n",
        "    NHID = 128\n",
        "    POOLING_RATIO = 0.5\n",
        "    DROPOUT_RATIO = 0.3\n",
        "\n",
        "    OUTER_LR     = 0.001\n",
        "    INNER_LR     = 0.01\n",
        "    STOP_LR      = 0.0001\n",
        "    WEIGHT_DECAY = 1E-05\n",
        "\n",
        "    MAX_STEP      = 15\n",
        "    MIN_STEP      = 5\n",
        "    STEP_TEST     = 15\n",
        "    FLEXIBLE_STEP = True\n",
        "    STEP_PENALITY = 0.001\n",
        "    USE_SCORE     = True\n",
        "    USE_GRAD      = False\n",
        "    USE_LOSS      = True\n",
        "\n",
        "    TRAIN_SHOT         = 10   # K-shot for training set\n",
        "    VAL_SHOT           = 10   # K-shot for validation (or test) set\n",
        "    TRAIN_QUERY        = 15   # Number of query for the training set\n",
        "    VAL_QUERY          = 15   # Number of query for the validation (or test) set\n",
        "    TRAIN_WAY          = 3    # N-way for training set\n",
        "    TEST_WAY           = 3    # N-way for test set\n",
        "    VAL_EPISODE        = 200  # Number of episodes for validation\n",
        "    TRAIN_EPISODE      = 200  # Number of episodes for training\n",
        "    BATCH_PER_EPISODES = 5    # How many batch per episode\n",
        "    EPOCHS             = 500  # How many epochs\n",
        "    PATIENCE           = 35\n",
        "    GRAD_CLIP          = 5\n",
        "\n",
        "    # Stop Control configurations\n",
        "    STOP_CONTROL_INPUT_SIZE = 2\n",
        "    STOP_CONTROL_HIDDEN_SIZE = 20"
      ],
      "metadata": {
        "id": "7vboF7-3VcvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "qs070YovWwAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scandir(root_path: str) -> List[str]:\n",
        "    \"\"\"Recursively scan a directory looking for files\"\"\"\n",
        "    root_path = os.path.abspath(root_path)\n",
        "    content = []\n",
        "    for file in os.listdir(root_path):\n",
        "        new_path = os.path.join(root_path, file)\n",
        "        if os.path.isfile(new_path):\n",
        "            content.append(new_path)\n",
        "            continue\n",
        "        \n",
        "        content += scandir(new_path)\n",
        "    \n",
        "    return content\n",
        "\n",
        "\n",
        "def download_zipped_data(url: str, path2extract: str, dataset_name: str) -> List[str]:\n",
        "    \"\"\"Download and extract a ZIP file from URL. Return the content filename\"\"\"\n",
        "    logging.debug(f\"--- Downloading from {url} ---\")\n",
        "    response = requests.get(url)\n",
        "\n",
        "    abs_path2extract = os.path.abspath(path2extract)\n",
        "    zip_path = os.path.join(abs_path2extract, f\"{dataset_name}.zip\")\n",
        "    with open(zip_path, mode=\"wb\") as iofile:\n",
        "        iofile.write(response.content)\n",
        "\n",
        "    # Extract the file\n",
        "    logging.debug(\"--- Extracting files from the archive ---\")\n",
        "    with zipfile.ZipFile(zip_path, mode=\"r\") as zip_ref:\n",
        "        zip_ref.extractall(abs_path2extract)\n",
        "\n",
        "    logging.debug(f\"--- Removing {zip_path} ---\")\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    return scandir(os.path.join(path2extract, dataset_name))\n",
        "\n",
        "\n",
        "def delete_data_folder(path2delete: str) -> None:\n",
        "    \"\"\"Delete the folder containing data\"\"\"\n",
        "    logging.debug(\"--- Removing Content Data ---\")\n",
        "    shutil.rmtree(path2delete)\n",
        "    logging.debug(\"--- Removed Finished Succesfully ---\")"
      ],
      "metadata": {
        "id": "hiPgKuQqXDNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def elapsed_time(func):\n",
        "    \"\"\"Just a simple wrapper for counting elapsed time from start to end\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        logging.debug(\"Elapsed Time: {:.6f}\".format(end - start))\n",
        "    \n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "7DFVuisvXFHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_with_pickle(path2save: str, content: Any) -> None:\n",
        "    \"\"\"Save content inside a .pickle file denoted by path2save\"\"\"\n",
        "    path2save = path2save + \".pickle\" if \".pickle\" not in path2save else path2save\n",
        "    with open(path2save, mode=\"wb\") as iostream:\n",
        "        pickle.dump(content, iostream)\n",
        "\n",
        "\n",
        "def load_with_pickle(path2load: str) -> Any:\n",
        "    \"\"\"Load a content from a .pickle file\"\"\"\n",
        "    with open(path2load, mode=\"rb\") as iostream:\n",
        "        return pickle.load(iostream)"
      ],
      "metadata": {
        "id": "fN6UJb7jXJ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "JmagKA3bXL1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}