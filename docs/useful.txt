--- DATA ---

|s| = N x K  -->  support set dimension
|q| = N x Q  -->  query set dimension

s, q share the same class space

G(train) = {(G(train, i), y(train, i))}  -->  train set
d = |G(train)|  -->  dimension of the train set
c = |unique(y(train, i), i=1...d)|  -->  number of unique classes of train set

c > N

epoch_size  -->  number of batches per epoch
batch_size  -->  number of training episodes per batch

--- COSTANTS ---

POOLING_RATIO = 0.5
DROPOUT_RATIO = 0.3

OUTER_LR     = 0.001
INNER_LR     = 0.01
STOP_LR      = 0.0001
WEIGHT_DECAY = 1E-05

MAX_STEP      = 15
MIN_STEP      = 5
STEP_TEST     = 15
FLEXIBLE_STEP = True
STEP_PENALITY = 0.001
USE_SCORE     = True
USE_GRAD      = False
USE_LOSS      = True

TRAIN_SHOT         = 10   # K-shot for training set
VAL_SHOT           = 10   # K-shot for validation (or test) set
TRAIN_QUERY        = 15   # Number of query for the training set
VAL_QUERY          = 15   # Number of query for the validation (or test) set
TRAIN_WAY          = 3    # N-way for training set
TEST_WAY           = 3    # N-way for test set
VAL_EPISODE        = 200  # Number of episodes for validation
TRAIN_EPISODE      = 200  # Number of episodes for training
BATCH_PER_EPISODES = 5    # How many batch per episode
EPOCHS             = 500  # How many epochs
PATIENCE           = 35
GRAD_CLIP          = 5

--- SUPPORT AND QUERY SET SAMPLER ---

function iter_sample_NKshot_with_Query(
	Data:
		- G(train)  --> train set
		- d --> dimension of the train set
		- c --> number of classes of the train set
		- N --> Number of classes to select
		- K --> Number of support sample per class
		- Q --> Number of query sample per class
		- epoch_size --> number of batches per epoch
){
	target_classes = random.sample(from=unique(y(train, i), i=1...d), size=N)

	for (i=1...epoch_size) do
	{
		n_way_k_shot_n_query = List()

		foreach (cl <- target_classes) do 
		{
			filtered_data = filter(data=G(train),by=Lambda(x, x.y == cl))
			f = |filtered_data|

			IMPORTANT: assert(f >= K + Q)

			selected_data = random.sample(from=filtered_data, size=(K + Q))
			n_way_k_shot_n_query.add(selected_data)
		}

		generate(support_data, query_data)
	}
}

--- TASK BATCH SAMPLER ---

function iter_task_batch_sampler(
	Data:
		- functions.iter_sample_NKshot_with_Query.PARAMETERS
		- batch_size --> number of batches
){
	few_shot_sampler = iter_sample_NKshot_with_Query(
		G(train), d, c, N, K, Q, epoch_size
	)

	mini_batches = List()
	foreach (counter, task in few_shot_sampler.iterate())
	{
		mini_batches.add(task)
		if (counter + 1) MOD batch_size == 0 {
			generate(mini_batches)
			mini_batches.clear()
		}
	}
}

--- CREATE BATCHES FROM DATA_BATCH ---

function create_batches_from_data_batch(
	Data:
		- data_batch --> batch of data
		- N --> Number of classes to select
		- K --> Number of support sample per class
		- Q --> Number of query sample per class
		- batch_size --> number of batches
	Return:
		- support_data_batch
		- query_data_batch
){
	support_data_batch = List()
	query_data_batch = List()

	for (i=1...batch_size) do
	{
		for (j=1...N) do
		{
			data_batch_per_batch = data_batch[i * N * (K + Q) : (i + 1) * N * (K + Q)]
			support_query_data = data_batch_per_batch[j * (K + Q) : (j + 1) * (K + Q)]
			support_data = support_query_data[0:K]
			query_data = support_query_data[K:K+Q]
			
			support_data_batch.add(support_data)
			query_data_batch.add(query_data)
		}
	}

	Return support_data_batch, query_data_batch
}

--- AS-MAML Algorithm ---

Algorithm:
	function AS-MAML(
		Input:
			- Task distribution p(T) over {(G_train, y_train)}
		
		Paramters:
			- Graph embedding parameters "e"
			- Classifier parameters "c"
			- Step control parametrs "s"
			- Learning Rates a1, a2, a3
		
		Output:
			- Trained parameters
	){
		initialize_random(a1, a2, a3)
		while (not converge) do
		{
			T_i = (D(train, sup), D(train, query)) <- random.sample(p(T))
			T <- get_adaptation_step()
			P' <- P = {e, c}  // Set fast adaptation parameters
			for (t=0 ... T) do
			{
				P' <- P' - a1 * Grad(P', L(T_i, f_P'))
				M_T <- compute_ANI()
				p(t) <- compute_stop_probability()
				Q(t) <- compute_reward(on=D(train, que))
			}

			P <- P - a2 * Grad(P', L(T_i, f_P'))
			for (t=0 ... T) do
			{
				s <- s + a3 * Q(t) * Grad(s, ln(p(t)))
			}
		}
	}

--- MAPPINGS PAPER TO MY CODE ---

support_labels  -->  tensor([[ ... ]])  -->  dim = 1 x (N  * K)
support_nodes  -->  tensor([[[ . ], [ . ], ..., [ . ]]])  -->  dim = 1 x num_attributes x 1
task_num  -->  dim(support_nodes).index(0) = 1


----


3 DATASETS
==========
training set, validation set = (original training set + original validation set).split(2)
	1. training set
	2. original validation set
	3. validation set


AS-MAML <- (trainin set, original validation set)
M-Evolve Augmentation <- (training set, validation set)